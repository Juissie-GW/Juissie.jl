var documenterSearchIndex = {"docs":
[{"location":"man/apikey/#Managing-API-Keys","page":"Managing API-Keys","title":"Managing API-Keys","text":"","category":"section"},{"location":"man/apikey/","page":"Managing API-Keys","title":"Managing API-Keys","text":"Secure management of secret keys is important. Every user should create a .env file in the project root where they add their API key(s), e.g.:","category":"page"},{"location":"man/apikey/","page":"Managing API-Keys","title":"Managing API-Keys","text":"OAI_KEY=ABC123","category":"page"},{"location":"man/apikey/","page":"Managing API-Keys","title":"Managing API-Keys","text":"These may be accessed using Julia via the DotEnv library. First, install DotEnv:","category":"page"},{"location":"man/apikey/","page":"Managing API-Keys","title":"Managing API-Keys","text":"import Pkg\nPkg.add(\"DotEnv\")","category":"page"},{"location":"man/apikey/","page":"Managing API-Keys","title":"Managing API-Keys","text":"Then, use it to access environmental variables from your .env file:","category":"page"},{"location":"man/apikey/","page":"Managing API-Keys","title":"Managing API-Keys","text":"using DotEnv\ncfg = DotEnv.config()\n\napi_key = cfg[\"OAI_KEY\"]","category":"page"},{"location":"man/apikey/","page":"Managing API-Keys","title":"Managing API-Keys","text":"Note that DotEnv looks for .env in the current directory, i.e. that of the calling function. If .env is in a different path, you have to provide it, e.g. DotEnv.config(YOUR_PATH_HERE)","category":"page"},{"location":"man/apikey/#Obtaining-an-OpenAI-API-Key","page":"Managing API-Keys","title":"Obtaining an OpenAI API Key","text":"","category":"section"},{"location":"man/apikey/","page":"Managing API-Keys","title":"Managing API-Keys","text":"Create an OpenAI account here.\nSet up billing information (each query has a small cost) here.\nCreate a new sectet key here.","category":"page"},{"location":"man/guide/#User-Guide","page":"User Guide","title":"User Guide","text":"","category":"section"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"Clone this repo\nNavigate into the cloned repo directory:","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"cd Juissie.jl","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"In general, we assume the user is running julia other other commands (e.g., jupyter notebook) from the root level of this project.","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"Open the Julia REPL by typing 'julia' into the terminal. Then, install the package dependencies:","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"using Pkg\nPkg.activate(\".\")\nPkg.instantiate()","category":"page"},{"location":"man/guide/#Verify-Setup","page":"User Guide","title":"Verify Setup","text":"","category":"section"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"From this repo's home directory, open the Julia REPL by typing 'julia' into the terminal. Then, try importing the Juissie module:","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"using Juissie","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"This should expose symbols like Corpus, Embedder, upsert_chunk, upsert_document, search, and embed.","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"Try instantiating one of the exported structs, like Corpus:","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"corpus = Corpus()","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"We can test the upsert and search functionality associated with Corpus like so:","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"Upsert some chunks to the vector database:","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"upsert_chunk(corpus, \"Hold me closer, tiny dancer.\", \"doc1\")\nupsert_chunk(corpus, \"Count the headlights on the highway.\", \"doc1\")\nupsert_chunk(corpus, \"Lay me down in sheets of linen.\", \"doc2\")\nupsert_chunk(corpus, \"Peter Piper picked a peck of pickled peppers. A peck of pickled peppers, Peter Piper picked.\", \"doc2\")","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"Search those chunks:","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"idx_list, doc_names, chunks, distances = search(\n    corpus, \n    \"tiny dancer\", \n    2\n)","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"The output should look like this:","category":"page"},{"location":"man/guide/","page":"User Guide","title":"User Guide","text":"([1, 3], [\"doc1\", \"doc2\"], [\"Hold me closer, tiny dancer.\", \"Lay me down in sheets of linen.\"], Vector{Float32}[[5.198073, 9.5337925]])","category":"page"},{"location":"#Juissie.jl","page":"Home","title":"Juissie.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"JUISSIE is a Julia-native semantic query engine, is versatile for integration as a package in software development workflows or through its desktop user interface.","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Juissie is a Julia-native semantic query engine. It can be used as a package in software development workflows, or via its desktop user interface.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Juissie was developed as a class project for CSCI 6221: Advanced Software Paradigms at The George Washington University.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Clone this repo\nNavigate into the cloned repo directory:","category":"page"},{"location":"","page":"Home","title":"Home","text":"cd Juissie","category":"page"},{"location":"","page":"Home","title":"Home","text":"In general, we assume the user is running the julia command, and all other commands (e.g., jupyter notebook), from the root level of this project.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Open the Julia REPL by typing julia into the terminal. Then, install the package dependencies:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.activate(\".\")\nPkg.resolve()\nPkg.instantiate()","category":"page"},{"location":"","page":"Home","title":"Home","text":"To use our generators, you may need an OpenAI API key see here. To run our demo Jupyter notebooks, you may need to setup Jupyter see here.","category":"page"},{"location":"#Verify-setup","page":"Home","title":"Verify setup","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"From this repo's home directory, open the Julia REPL by typing julia into the terminal. Then, try importing the Juissie module:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Juissie","category":"page"},{"location":"","page":"Home","title":"Home","text":"This should expose symbols like Corpus, Embedder, upsert_chunk, upsert_document, search, and embed.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Try instantiating one of the exported struct, like Corpus:","category":"page"},{"location":"","page":"Home","title":"Home","text":"corpus = Corpus()","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can test the upsert and search functionality associated with Corpus like so:","category":"page"},{"location":"","page":"Home","title":"Home","text":"upsert_chunk(corpus, \"Hold me closer, tiny dancer.\", \"doc1\")\nupsert_chunk(corpus, \"Count the headlights on the highway.\", \"doc1\")\nupsert_chunk(corpus, \"Lay me down in sheets of linen.\", \"doc2\")\nupsert_chunk(corpus, \"Peter Piper picked a peck of pickled peppers. A peck of pickled peppers, Peter Piper picked.\", \"doc2\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Search those chunks:","category":"page"},{"location":"","page":"Home","title":"Home","text":"idx_list, doc_names, chunks, distances = search(\n    corpus, \n    \"tiny dancer\", \n    2\n)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The output should look like this:","category":"page"},{"location":"","page":"Home","title":"Home","text":"([1, 3], [\"doc1\", \"doc2\"], [\"Hold me closer, tiny dancer.\", \"Lay me down in sheets of linen.\"], Vector{Float32}[[5.198073, 9.5337925]])","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"#Desktop-UI","page":"Home","title":"Desktop UI","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Navigate to the root directory of this repository (Juissie.jl), enter the following into the command line, and press the enter/return key:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia src/Frontend.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"This will launch our application:","category":"page"},{"location":"","page":"Home","title":"Home","text":"<img src=\"https://raw.githubusercontent.com/Juissie-GW/Juissie.jl/main/assets/ui1.png\" alt=\"ui1\" width=\"500\"/>","category":"page"},{"location":"#API-Keys","page":"Home","title":"API Keys","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Juissie's default generator requires an OpenAI API key. This can be provided manually in the UI (see the API Key tab of the Corpus Manager) or passed as an argument when initializing the generator. The preferred method, however, is to stash your API key in a .env file.","category":"page"},{"location":"#Obtaining-an-OpenAI-API-Key","page":"Home","title":"Obtaining an OpenAI API Key","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Create an OpenAI account here.\nSet up billing information (each query has a small cost) here.\nCreate a new secret key here.","category":"page"},{"location":"#Managing-API-Keys","page":"Home","title":"Managing API Keys","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Secure management of secret keys is important. Every user should create a .env file in the project root where they add their API key(s), e.g.:","category":"page"},{"location":"","page":"Home","title":"Home","text":"OAI_KEY=ABC123","category":"page"},{"location":"","page":"Home","title":"Home","text":"These may be accessed using Julia via the DotEnv library. First, run the julia command in a terminal. Then install DotEnv:","category":"page"},{"location":"","page":"Home","title":"Home","text":"import Pkg\nPkg.add(\"DotEnv\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Then, use it to access environmental variables from your .env file:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using DotEnv\ncfg = DotEnv.config()\n\napi_key = cfg[\"OAI_KEY\"]","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that DotEnv looks for .env in the current directory, i.e. that of where you called julia from.  If .env is in a different path, you have to provide it, e.g. DotEnv.config(YOUR_PATH_HERE). If you are invoking Juissie from the root directory of this repo (typical), this means the .env should be placed there.","category":"page"},{"location":"#Functions","page":"Home","title":"Functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modules = [Juissie, Juissie.Generation.SemanticSearch.Embedding, Juissie.SemanticSearch.Backend, Juissie.Generation.SemanticSearch.Backend, Juissie.Generation.SemanticSearch.Backend.TextUtils, Juissie.SemanticSearch.Backend.TxtReader, Juissie.Generation, Juissie.SemanticSearch.Backend.PdfReader, Juissie.Generation.SemanticSearch.Backend.TxtReader,Juissie.SemanticSearch.Embedding, Juissie.SemanticSearch.TextUtils, Juissie.Generation.SemanticSearch.Backend.PdfReader, Juissie.SemanticSearch.Backend.Embedding, Juissie.SemanticSearch.Backend.TextUtils, Juissie.Generation.SemanticSearch.Backend.Embedding, Juissie.Generation.SemanticSearch.TextUtils, Generation.SemanticSearch.Backend, Generation, Generation.SemanticSearch.Backend.TextUtils]\n\nOrder   = [:function, :type]","category":"page"},{"location":"#Juissie.Generation.SemanticSearch.Embedding.embed-Tuple{Juissie.Generation.SemanticSearch.Embedding.Embedder, String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Embedding.embed","text":"function embed(embedder::Embedder, text::String)::AbstractVector\n\nEmbeds a textual sequence using a provided model\n\nParameters\n\nembedder : Embedder     an initialized Embedder struct text : String     the text sequence you want to embed\n\nNotes\n\nThis is sort of like a class method for the Embedder\n\nJulia has something called multiple dispatch that can be used to  make this cleaner, but I'm going to handle that at a later times\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Embedding.embed_from_bert-Tuple{Juissie.Generation.SemanticSearch.Embedding.Embedder, String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Embedding.embed_from_bert","text":"function embed_from_bert(embedder::Embedder, text::String)\n\nEmbeds a textual sequence using a provided Bert model\n\nParameters\n\nembedder : Embedder     an initialized Embedder struct     the associated model and tokenizer should be Bert-specific text : String     the text sequence you want to embed\n\nreturn : cls_embedding     The results from passing the text through the encoder, throught the model,     and after stripping \n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Embedding.Embedder","page":"Home","title":"Juissie.Generation.SemanticSearch.Embedding.Embedder","text":"struct Embedder\n\nA struct for holding a model and a tokenizer\n\nAttributes\n\ntokenizer : a tokenizer object, e.g. BertTextEncoder     maps your string to tokens the model can understand model : a model object, e.g. HGFBertModel     the actual model architecture and weights to perform inference with\n\nNotes\n\nYou can get class-like behavior in Julia by defining a struct and functions that operate on that struct.\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.Generation.SemanticSearch.Embedding.Embedder-Tuple{String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Embedding.Embedder","text":"function Embedder(model_name::String)\n\nFunction to initialize an Embedder struct from a HuggingFace model path.\n\nParameters\n\nmodel_name : String     a path to a HuggingFace-hosted model     e.g. \"BAAI/bge-small-en-v1.5\"\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.index-Tuple{Corpus}","page":"Home","title":"Juissie.SemanticSearch.Backend.index","text":"function index(corpus::Corpus)\n\nConstructs the HNSW vector index from the data available. If the corpus has a corpus_name, then we also save the new index to disk. Must be run before searching.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.load_corpus-Tuple{String}","page":"Home","title":"Juissie.SemanticSearch.Backend.load_corpus","text":"function load_corpus(corpus_name)\n\nLoads an already-initialized corpus from its associated \"artifacts\" (relational database, vector index, and informational json).\n\nParameters\n\ncorpus_name : str     the name of your EXISTING vector database\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.search","page":"Home","title":"Juissie.SemanticSearch.Backend.search","text":"function search(corpus::Corpus, query::String, k::Int=5)\n\nPerforms approximate nearest neighbor search to find the items in the vector index closest to the query.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use query : str     The text you want to search, e.g. your question     We embed this and perform semantic retrieval against the vector db k : int     The number of nearest-neighbor vectors to fetch\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.SemanticSearch.Backend.upsert_chunk-Tuple{Corpus, String, String}","page":"Home","title":"Juissie.SemanticSearch.Backend.upsert_chunk","text":"function upsert_chunk(corpus::Corpus, chunk::String, doc_name::String)\n\nGiven a new chunk of text, get embedding and insert into our vector DB. Not actually a full upsert, because we have to reindex later. Process:\n\nGenerate an embedding for the text\nInsert metadata into database\nIncrement idx counter\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use chunk : str     This is the text content of the chunk you want to upsert docname : str     The name of the document that chunk is from. For instance, if you      were upserting all the chunks in an academic paper, docname might     be the name of that paper\n\nNotes\n\nIf the vectors have been indexed, this de-indexes them (i.e., they need to be indexed again). Currently, we handle this by setting hnsw to  nothing so that it gets caught later in search.\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.upsert_document-Tuple{Corpus, String, String}","page":"Home","title":"Juissie.SemanticSearch.Backend.upsert_document","text":"function upsert_document(corpus::Corpus, doc_text::String, doc_name::String)\n\nUpsert a whole document (i.e., long string). Does so by splitting the document into appropriately-sized chunks so no chunk exceeds the embedder's tokenization max sequence length, while prioritizing sentence endings.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use doctext : str     A long string you want to upsert. We will break this into chunks and     upsert each chunk. docname : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.upsert_document-Tuple{Corpus, Vector{String}, String}","page":"Home","title":"Juissie.SemanticSearch.Backend.upsert_document","text":"function upsert_document(corpus::Corpus, documents::Vector{String}, doc_name::String)\n\nUpsert a collection of documents (i.e., a vector of long strings). Does so by upserting each entry of the provided documents vector (which in turn will chunkify, each document further into appropriately sized chunks).\n\nSee the upsert_document(...) above for more details\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use documents : Vector{String}     a collection of long strings to upsert. doc_name : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.upsert_document_from_pdf-Tuple{Corpus, String, String}","page":"Home","title":"Juissie.SemanticSearch.Backend.upsert_document_from_pdf","text":"function upsert_document_from_pdf(corpus::Corpus, filePath::String, doc_name::String)\n\nUpsert all the data in a PDF file into the provided corpus. See the upsert_document(...) above for more details.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use filePath : String     The path to the PDF file to read doc_name : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.upsert_document_from_txt-Tuple{Corpus, String, String}","page":"Home","title":"Juissie.SemanticSearch.Backend.upsert_document_from_txt","text":"function upsert_document_from_txt(corpus::Corpus, filePath::String, doc_name::String)\n\nUpsert all the data from the text file into the provided corpus.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use filePath : String     The path to the txt file to read doc_name : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.upsert_document_from_url","page":"Home","title":"Juissie.SemanticSearch.Backend.upsert_document_from_url","text":"function upsert_document_from_url(corpus::Corpus, url::String, doc_name::String, elements::Array{String}=[\"h1\", \"h2\", \"p\"])\n\nExtracts element-tagged text from HTML and upserts as a document.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use url : String     The url you want to scrape for text doc_name : str     The name of the document the content is from elements : Array{String}     A list of HTML elements you want to pull the text from\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.SemanticSearch.Backend.Corpus","page":"Home","title":"Juissie.SemanticSearch.Backend.Corpus","text":"struct Corpus\n\nBasically a vector database. It will have these attributes:\n\na relational database (SQLite)\na vector index (HNSW)\nan embedder (via Embedding.jl)\n\nAttributes\n\ncorpusname : String or Nothing     this is the name of your corpus and will be used to access saved          corpuses     if Nothing, we can't save/load and everything will be in-memory db : a SQLite.DB connection object     this is a real relational database to store metadata (e.g. chunk text, doc name) hnsw : Hierarchical Navigable Small World object     this is our searchable vector index embedder : Embedder     an initialized Embedder struct maxseqlen : int     The maximum number of tokens per chunk.     This should be the max sequence length of the tokenizer data : Vector{Any}     The embeddings get stored here before we create the vector index nextidx : int     stores the index we'll use for the next-upserted chunk\n\nNotes\n\nThe struct is mutable because we want to be able to change things like incrementing next_idx.\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.SemanticSearch.Backend.Corpus-2","page":"Home","title":"Juissie.SemanticSearch.Backend.Corpus","text":"function Corpus(corpus_name::String, embedder_model_path::String=\"BAAI/bge-small-en-v1.5\")\n\nInitializes a Corpus struct.\n\nIn particular, does the following:\n\nInitializes an embedder object\nCreates a SQLite databse with the corpus name. It should have:\n\nrow-wise primary key uuid\ndoc_name representing the parent document\nchunk text \n\nWe can add more metadata later, if desired\n\nParameters\n\ncorpusname : str or nothing     the name that you want to give the database     optional. if left as nothing, we use an in-memory database embeddermodelpath : str     a path to a HuggingFace-hosted model     e.g. \"BAAI/bge-small-en-v1.5\" maxseq_len : int     The maximum number of tokens per chunk.     This should be the max sequence length of the tokenizer\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.Generation.SemanticSearch.Backend.index-Tuple{Juissie.Generation.SemanticSearch.Backend.Corpus}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.index","text":"function index(corpus::Corpus)\n\nConstructs the HNSW vector index from the data available. If the corpus has a corpus_name, then we also save the new index to disk. Must be run before searching.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.load_corpus-Tuple{String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.load_corpus","text":"function load_corpus(corpus_name)\n\nLoads an already-initialized corpus from its associated \"artifacts\" (relational database, vector index, and informational json).\n\nParameters\n\ncorpus_name : str     the name of your EXISTING vector database\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.search","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.search","text":"function search(corpus::Corpus, query::String, k::Int=5)\n\nPerforms approximate nearest neighbor search to find the items in the vector index closest to the query.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use query : str     The text you want to search, e.g. your question     We embed this and perform semantic retrieval against the vector db k : int     The number of nearest-neighbor vectors to fetch\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.SemanticSearch.Backend.upsert_chunk-Tuple{Juissie.Generation.SemanticSearch.Backend.Corpus, String, String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.upsert_chunk","text":"function upsert_chunk(corpus::Corpus, chunk::String, doc_name::String)\n\nGiven a new chunk of text, get embedding and insert into our vector DB. Not actually a full upsert, because we have to reindex later. Process:\n\nGenerate an embedding for the text\nInsert metadata into database\nIncrement idx counter\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use chunk : str     This is the text content of the chunk you want to upsert docname : str     The name of the document that chunk is from. For instance, if you      were upserting all the chunks in an academic paper, docname might     be the name of that paper\n\nNotes\n\nIf the vectors have been indexed, this de-indexes them (i.e., they need to be indexed again). Currently, we handle this by setting hnsw to  nothing so that it gets caught later in search.\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.upsert_document-Tuple{Juissie.Generation.SemanticSearch.Backend.Corpus, String, String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.upsert_document","text":"function upsert_document(corpus::Corpus, doc_text::String, doc_name::String)\n\nUpsert a whole document (i.e., long string). Does so by splitting the document into appropriately-sized chunks so no chunk exceeds the embedder's tokenization max sequence length, while prioritizing sentence endings.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use doctext : str     A long string you want to upsert. We will break this into chunks and     upsert each chunk. docname : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.upsert_document-Tuple{Juissie.Generation.SemanticSearch.Backend.Corpus, Vector{String}, String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.upsert_document","text":"function upsert_document(corpus::Corpus, documents::Vector{String}, doc_name::String)\n\nUpsert a collection of documents (i.e., a vector of long strings). Does so by upserting each entry of the provided documents vector (which in turn will chunkify, each document further into appropriately sized chunks).\n\nSee the upsert_document(...) above for more details\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use documents : Vector{String}     a collection of long strings to upsert. doc_name : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.upsert_document_from_pdf-Tuple{Juissie.Generation.SemanticSearch.Backend.Corpus, String, String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.upsert_document_from_pdf","text":"function upsert_document_from_pdf(corpus::Corpus, filePath::String, doc_name::String)\n\nUpsert all the data in a PDF file into the provided corpus. See the upsert_document(...) above for more details.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use filePath : String     The path to the PDF file to read doc_name : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.upsert_document_from_txt-Tuple{Juissie.Generation.SemanticSearch.Backend.Corpus, String, String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.upsert_document_from_txt","text":"function upsert_document_from_txt(corpus::Corpus, filePath::String, doc_name::String)\n\nUpsert all the data from the text file into the provided corpus.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use filePath : String     The path to the txt file to read doc_name : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.upsert_document_from_url","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.upsert_document_from_url","text":"function upsert_document_from_url(corpus::Corpus, url::String, doc_name::String, elements::Array{String}=[\"h1\", \"h2\", \"p\"])\n\nExtracts element-tagged text from HTML and upserts as a document.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use url : String     The url you want to scrape for text doc_name : str     The name of the document the content is from elements : Array{String}     A list of HTML elements you want to pull the text from\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.SemanticSearch.Backend.Corpus","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.Corpus","text":"struct Corpus\n\nBasically a vector database. It will have these attributes:\n\na relational database (SQLite)\na vector index (HNSW)\nan embedder (via Embedding.jl)\n\nAttributes\n\ncorpusname : String or Nothing     this is the name of your corpus and will be used to access saved          corpuses     if Nothing, we can't save/load and everything will be in-memory db : a SQLite.DB connection object     this is a real relational database to store metadata (e.g. chunk text, doc name) hnsw : Hierarchical Navigable Small World object     this is our searchable vector index embedder : Embedder     an initialized Embedder struct maxseqlen : int     The maximum number of tokens per chunk.     This should be the max sequence length of the tokenizer data : Vector{Any}     The embeddings get stored here before we create the vector index nextidx : int     stores the index we'll use for the next-upserted chunk\n\nNotes\n\nThe struct is mutable because we want to be able to change things like incrementing next_idx.\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.Generation.SemanticSearch.Backend.Corpus-2","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.Corpus","text":"function Corpus(corpus_name::String, embedder_model_path::String=\"BAAI/bge-small-en-v1.5\")\n\nInitializes a Corpus struct.\n\nIn particular, does the following:\n\nInitializes an embedder object\nCreates a SQLite databse with the corpus name. It should have:\n\nrow-wise primary key uuid\ndoc_name representing the parent document\nchunk text \n\nWe can add more metadata later, if desired\n\nParameters\n\ncorpusname : str or nothing     the name that you want to give the database     optional. if left as nothing, we use an in-memory database embeddermodelpath : str     a path to a HuggingFace-hosted model     e.g. \"BAAI/bge-small-en-v1.5\" maxseq_len : int     The maximum number of tokens per chunk.     This should be the max sequence length of the tokenizer\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.Generation.SemanticSearch.Backend.TextUtils.chunkify","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.TextUtils.chunkify","text":"function chunkify(text::String, tokenizer, sequence_length::Int=512)\n\nSplits a provided text (e.g. paragraph) into chunks that are each as many sentences as possible while keeping the chunk's token lenght below the sequence_length. This ensures that each chunk can be fully encoded by the embedder.\n\nParameters\n\ntext : String     The text you want to split into chunks. tokenizer : a tokenizer object, e.g. BertTextEncoder     The tokenizer you will be using sequence_length : Int     The maximum number of tokens per chunk.     Ideally, should correspond to the max sequence length of the tokenizer\n\nExample Usage\n\n>>> chunkify(\n    '''Hold me closer, tiny dancer. Count the headlights on the highway. Lay me down in sheets of linen. Peter Piper picked a peck of pickled peppers. A peck of pickled peppers Peter Piper picked.\n    ''', \n    corpus.embedder.tokenizer, \n    20\n)\n\n4-element Vector{Any}:\n\"Hold me closer, tiny dancer. Count the headlights on the highway.\"\n\"Lay me down in sheets of linen.\"\n\"Peter Piper picked a peck of pickled peppers.\"\n\"A peck of pickled peppers Peter Piper picked.\"\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.SemanticSearch.Backend.TextUtils.get_files_path-Tuple{}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.TextUtils.get_files_path","text":"function get_files_path()\n\nSimple function to return the path to the files subdirectory.\n\nExample Usage\n\ntestbinpath = getfilespath()*\"test.bin\"\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.TextUtils.read_html_url","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.TextUtils.read_html_url","text":"read_html_url(url::String, elements::Array{String})\n\nReturns a string of text from the provided HTML elements on a webpage.\n\nParameters\n\nurl : String     the url you want to read elements : Array{String}     html elements to look for in the web page, e.g. [\"h1\", \"p\"].\n\nNotes\n\nDefaults to extracting headers and paragraphs\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.SemanticSearch.Backend.TextUtils.sentence_splitter-Tuple{String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.TextUtils.sentence_splitter","text":"function sentence_splitter(text::String)\n\nUses basic regex to divide a provided text (e.g. paragraph) into sentences.\n\nParameters\n\ntext : String     The text you want to split into sentences.\n\nNotes\n\nRegex is hard to read. The first part looks for spaces following  end-of-sentence punctuation. The second part matches at the end of the string.\n\nRegex in Julia uses an r identifier prefix.\n\nReferences\n\nhttps://www.geeksforgeeks.org/regular-expressions-in-julia/\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.TxtReader.appendToFile-Tuple{String, String}","page":"Home","title":"Juissie.SemanticSearch.Backend.TxtReader.appendToFile","text":"Append the given contents into a file specified at filename. A new file will be created if filename doesn't already exist.\n\nNOTE: No ' ' newline character will be appended. It is the  caller's responsibility to decide if the contents should have a ' ' newline character or not. \n\nParameters\n\nfilename: String     The name of the file to open. Relative file paths are evaluated from     the directory where the julia command was run. Typically the root level     of the project contents: String     The exact text to append into the file.\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.TxtReader.getAllTextInFile-Tuple{String}","page":"Home","title":"Juissie.SemanticSearch.Backend.TxtReader.getAllTextInFile","text":"Open the provided filename, load all the data into memory, and return. This function will also manage the file socket open(...) close(...) properly. If there was an error in opening or reading the file then the empty  string will be returned\n\nParameters\n\nfilename: String     The name of the file to open. Relative file paths are evaluated from     the directory where the julia command was run. Typically the root level     of the project\n\nReturns: String     The entire contets of the file, or an empty string if there was an issue\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.TxtReader.splitFileIntoParts-Tuple{String, String, Int64}","page":"Home","title":"Juissie.SemanticSearch.Backend.TxtReader.splitFileIntoParts","text":"A simple script that allows a user to split a large file into multiple smaller files.  This will create splits # of children files, each with a file size ~1/splits  of the origional target file.\n\nParameters\n\nfileToSplit : String     The name of the file to read and split into multiple parts.     If an absolute file path is given then that will be     used. Otherwise, relative file paths are evaluated from the location that     the julia command was run from (typically the root level of this project) outputFileNameBase : String     The template for the name of the children split-out files.      Each split out file with have the format of <outputFileNameBase>_<#>      where # starts at 1 and increments by 1 for each subsequent file.      There will be splits number of children files splits : Int     How many children files should be created?\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.build_full_query","page":"Home","title":"Juissie.Generation.build_full_query","text":"function build_full_query(query::String, context::OptionalContext=nothing)\n\nGiven a query and a list of contextual chunks, construct a full query incorporating both.\n\nParameters\n\nquery : String     the main instruction or query string context : OptionalContext, which is Union{Vector{String}, Nothing}     optional list of chunks providing additional context for the query\n\nNotes\n\nWe use the Alpaca prompt, found here: https://github.com/tatsu-lab/stanford_alpaca with minor modifications that reflect our response preferences.\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.check_oai_key_format-Tuple{String}","page":"Home","title":"Juissie.Generation.check_oai_key_format","text":"function check_oai_key_format(key::String)\n\nUses regex to check if a provided string is in the expected format of an OpenAI     API Key\n\nParameters\n\nkey : String     the key you want to check\n\nNotes\n\nSee here for more on the regex:\n\nhttps://en.wikibooks.org/wiki/IntroducingJulia/Stringsandcharacters#Findingandreplacingthingsinsidestrings\n\nUses format rule provided here:\n\nhttps://github.com/secretlint/secretlint/issues/676\nhttps://community.openai.com/t/what-are-the-valid-characters-for-the-apikey/288643\n\nNote that this only checks the key format, not whether the key is valid or has not  been revoked.\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.generate","page":"Home","title":"Juissie.Generation.generate","text":"generate(generator::Union{OAIGenerator, Nothing}, query::String, context::OptionalContext=nothing, temperature::Float64=0.7)\n\nGenerate a response based on a given query and optional context using the specified OAIGenerator. This function constructs a full query, sends it to the OpenAI API, and returns the generated response.\n\nParameters\n\ngenerator : Union{OAIGenerator, Nothing}     an initialized generator (e..g OAIGenerator)     leaving this as a union with nothing to note that we may want to support other      generator types in the future (e.g. HFGenerator, etc.) query : String     the main query string. This is basically your question context : OptionalContext, which is Union{Vector{String}, Nothing}     optional list of contextual chunk strings to provide the generator additional      context for the query. Ultimately, these will be coming from our vector DB temperature : Float64     controls the stochasticity of the output generated by the model\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.generate_with_corpus","page":"Home","title":"Juissie.Generation.generate_with_corpus","text":"function generate_with_corpus(generator::Union{OAIGenerator, Nothing}, corpus::Corpus, query::String, k::Int=5, temperature::Float64=0.7)\n\nParameters\n\ngenerator : Union{OAIGenerator, Nothing}     an initialized generator (e..g OAIGenerator)     leaving this as a union with nothing to note that we may want to support other      generator types in the future (e.g. HFGenerator, etc.) corpus : an initialized Corpus object     the corpus / \"vector database\" you want to use query : String     the main instruction or query string. This is basically your question k : int     The number of nearest-neighbor vectors to fetch from the corpus to build your context temperature : Float64     controls the stochasticity of the output generated by the model\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.load_OAIGeneratorWithCorpus","page":"Home","title":"Juissie.Generation.load_OAIGeneratorWithCorpus","text":"function load_OAIGeneratorWithCorpus(corpus_name::String, auth_token::Union{String, Nothing}=nothing)\n\nLoads an existing corpus and uses it to initialize an GeneratorWithCorpus\n\nParameters\n\ncorpusname : str     the name that you want to give the database authtoken :: Union{String, Nothing}     this is your OPENAI API key. You can either pass it explicitly as a string     or leave this argument as nothing. In the latter case, we will look in your     environmental variables for \"OAI_KEY\"\n\nNotes\n\ncorpusname is ordered first because Julia uses positional arguments and  authtoken is optional.\n\nWhen instantiating a new OAIGenerator in an externally-viewable setting (e.g. notebooks committed to GitHub or a public demo), it is important to place a semicolon after the command, e.g.  '''generator=loadOAIGeneratorWithCorpus(\"greekphilosophers\");''' to ensure that your OAI API key is not inadvertently shared.\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.upsert_chunk_to_generator-Tuple{Juissie.Generation.GeneratorWithCorpus, String, String}","page":"Home","title":"Juissie.Generation.upsert_chunk_to_generator","text":"function upsert_chunk_to_generator(generator::GeneratorWithCorpus, chunk::String, doc_name::String)\n\nEquivalent to Backend.upsert_chunk, but takes a GeneratorWithCorpus instead of a Corpus.\n\nParameters\n\ngenerator : any struct that subtypes GeneratorWithCorpus     the generator (with corpus) you want to use chunk : str     This is the text content of the chunk you want to upsert docname : str     The name of the document that chunk is from. For instance, if you      were upserting all the chunks in an academic paper, docname might     be the name of that paper\n\nNotes\n\nOne would expect Julia's multiple dispatch to allow us to call this upsertchunk, but not so. The conflict arises in Juissie, where  we would have both SemanticSearch and Generation exporting  upsertchunk. This means any uses of it in Juissie must be  qualified, and without doing so, neither actually gets defined.\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.upsert_document_from_url_to_generator","page":"Home","title":"Juissie.Generation.upsert_document_from_url_to_generator","text":"function upsert_document_from_url_to_generator(generator::GeneratorWithCorpus, url::String, doc_name::String, elements::Array{String}=[\"h1\", \"h2\", \"p\"])\n\nEquivalent to Backend.upsertdocumentfrom_url, but takes a  GeneratorWithCorpus instead of a Corpus.\n\nParameters\n\ngenerator : any struct that subtypes GeneratorWithCorpus     the generator (with corpus) you want to use url : String     The url you want to scrape for text doc_name : str     The name of the document the content is from elements : Array{String}     A list of HTML elements you want to pull the text from\n\nNotes\n\nSee note for upsertchunkto_generator - same idea.\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.upsert_document_to_generator-Tuple{Juissie.Generation.GeneratorWithCorpus, String, String}","page":"Home","title":"Juissie.Generation.upsert_document_to_generator","text":"function upsert_document_to_generator(generator::GeneratorWithCorpus, doc_text::String, doc_name::String)\n\nEquivalent to Backend.upsert_document, but takes a GeneratorWithCorpus instead of a Corpus.\n\nParameters\n\ngenerator : any struct that subtypes GeneratorWithCorpus     the generator (with corpus) you want to use doctext : str     A long string you want to upsert. We will break this into chunks and     upsert each chunk. docname : str     The name of the document the content is from\n\nNotes\n\nSee note for upsertchunkto_generator - same idea.\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.OAIGenerator","page":"Home","title":"Juissie.Generation.OAIGenerator","text":"function OAIGenerator(auth_token::Union{String, Nothing})\n\nInitializes an OAIGenerator struct.\n\nParameters\n\nauthtoken :: Union{String, Nothing}     this is your OPENAI API key. You can either pass it explicitly as a string     or leave this argument as nothing. In the latter case, we will look in your     environmental variables for \"OAIKEY\"\n\nNotes\n\nWhen instantiating a new OAIGenerator in an externally-viewable setting (e.g. notebooks committed to GitHub or a public demo), it is important to place a semicolon after the command, e.g.  '''generator=loadOAIGeneratorWithCorpus(\"greekphilosophers\");''' to ensure that your OAI API key is not inadvertently shared.\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.Generation.OAIGenerator-2","page":"Home","title":"Juissie.Generation.OAIGenerator","text":"struct OAIGenerator\n\nA struct for handling natural language generation via OpenAI's gpt-3.5-turbo completion endpoint.\n\nAttributes\n\nurl : String     the URL of the OpenAI API endpoint header : Vector{Pair{String, String}}     key-value pairs representing the HTTP headers for the request body : Dict{String, Any}     this is the JSON payload to be sent in the body of the request\n\nNotes\n\nAll natural language generation should be done via a \"Generator\" object of some kind for consistency. In the future, if we  decide to host a model locally or something, we might do that via a HFGenerator struct.\n\nWhen instantiating a new OAIGenerator in an externally-viewable setting (e.g. notebooks committed to GitHub or a public demo), it is important to place a semicolon after the command, e.g.  '''generator=loadOAIGeneratorWithCorpus(\"greekphilosophers\");''' to ensure that your OAI API key is not inadvertently shared.\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.Generation.OAIGeneratorWithCorpus","page":"Home","title":"Juissie.Generation.OAIGeneratorWithCorpus","text":"function OAIGeneratorWithCorpus(auth_token::Union{String, Nothing}=nothing, corpus::Corpus)\n\nInitializes an OAIGeneratorWithCorpus.\n\nParameters\n\ncorpusname : str or nothing     the name that you want to give the database     optional. if left as nothing, we use an in-memory database authtoken :: Union{String, Nothing}     this is your OPENAI API key. You can either pass it explicitly as a string     or leave this argument as nothing. In the latter case, we will look in your     environmental variables for \"OAIKEY\" embeddermodelpath : str     a path to a HuggingFace-hosted model     e.g. \"BAAI/bge-small-en-v1.5\" maxseq_len : int     The maximum number of tokens per chunk.     This should be the max sequence length of the tokenizer\n\nNotes\n\nWhen instantiating a new OAIGenerator in an externally-viewable setting (e.g. notebooks committed to GitHub or a public demo), it is important to place a semicolon after the command, e.g.  '''generator=loadOAIGeneratorWithCorpus(\"greekphilosophers\");''' to ensure that your OAI API key is not inadvertently shared.\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.Generation.OAIGeneratorWithCorpus-2","page":"Home","title":"Juissie.Generation.OAIGeneratorWithCorpus","text":"struct OAIGeneratorWithCorpus\n\nLike OAIGenerator, but has a corpus attached.\n\nAttributes\n\nurl : String     the URL of the OpenAI API endpoint header : Vector{Pair{String, String}}     key-value pairs representing the HTTP headers for the request body : Dict{String, Any}     this is the JSON payload to be sent in the body of the request corpus : an initialized Corpus object     the corpus / \"vector database\" you want to use\n\nNotes\n\nWhen instantiating a new OAIGenerator in an externally-viewable setting (e.g. notebooks committed to GitHub or a public demo), it is important to place a semicolon after the command, e.g.  '''generator=loadOAIGeneratorWithCorpus(\"greekphilosophers\");''' to ensure that your OAI API key is not inadvertently shared.\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.SemanticSearch.Backend.PdfReader.bufferToString!-Tuple{IOBuffer}","page":"Home","title":"Juissie.SemanticSearch.Backend.PdfReader.bufferToString!","text":"Extract the contents of the buffer and convert it into a string object\n\nWARNING: This function will clear out the contents of the buffer\n\nParameters\n\nbuff : The buffer to clear, it's contents will be returned as a string\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.PdfReader.getAllTextInPDF","page":"Home","title":"Juissie.SemanticSearch.Backend.PdfReader.getAllTextInPDF","text":"Extract all the text data from the provided pdf file.\n\nOpen the pdf at the provided file location, extract all the text data from it (as far as possible), and return that text data as a vector of strings. Each entry in the rsult vector is the appended sum of some number of pages in the PDF. 100 Pages per entry is default. For example, the getAllTextInPDF(...)[0] will be a  long string containing 100 pages worth of data. The next entry represents the next 100  pages, etc.\n\nNOTE: This function is a \"best effort\" function, meaning that it will try to extract as many pages as it can. But if there are pages that are invalid, or otherwise can not be properly parsed then they will simply be ignored and not included in the  returned strings.\n\nParameters\n\nfileLocation : The full path to the PDF file to open. This should be relative from where the julia command has been run (not relative to this source file)\n\npagesPerEntry : How many pages should be collected into the buffere before turning it into an entry in the result vector.\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.SemanticSearch.Backend.PdfReader.getPagesFromPdf-Tuple{PDFIO.PD.PDDocImpl, Number, Number}","page":"Home","title":"Juissie.SemanticSearch.Backend.PdfReader.getPagesFromPdf","text":"Collect and return all the text data found in the pdf file found in the provided page range.\n\nUsing the provided PDF Handel, loop over all the pages in the range and attempt to extract the text data. All the collected data will be returned.\n\nThe specific pages to read are defined by [firstPageInclusive, lastPageInclusive] which (naturally) defines an inclusive range. Meaning the first and last page number will be included in the returned string. These ranges SHOULD be valid (ie, in the range [1, MaxPageCount]) but error checking will coerce the values to a proper range.\n\nParameters\n\npdfHandel : The PDF file to extract data from firstPageInclusive : The first page in the range to read lastPageInclusive : The last page in the range to read\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.PdfReader.getPagesFromPdf-Tuple{String, Number, Number}","page":"Home","title":"Juissie.SemanticSearch.Backend.PdfReader.getPagesFromPdf","text":"Collect and return all the text data found in the pdf file found in the provided page range.\n\nUsing the provided file path, open the PDF and loop over all the pages in the range and attempt to extract the text data. All the collected data will be returned.\n\nThe specific pages to read are defined by [firstPageInclusive, lastPageInclusive] which (naturally) defines an inclusive range. Meaning the first and last page number will be included in the returned string. These ranges SHOULD be valid (ie, in the range [1, MaxPageCount]) but error checking will coerce the values to a proper range.\n\nParameters\n\nfileLocation : The location of the PDF to read firstPageInclusive : The first page in the range to read lastPageInclusive : The last page in the range to read\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.PdfReader.getPagesInPDF_All-Tuple{String}","page":"Home","title":"Juissie.SemanticSearch.Backend.PdfReader.getPagesInPDF_All","text":"Extract all the text data from the provided pdf file.\n\nOpen the pdf at the provided file location, extract all the text data from it (as far as possible), and return that text data as a vector of strings. Each entry in the result vector is the data from a single page of the PDF file.\n\nNOTE: This function is a \"best effort\" function, meaning that it will try to extract as many pages as it can. But if there are pages that are invalid, or otherwise can not be properly parsed then they will simply be ignored and not included in the  returned strings.\n\nParameters\n\nfileLocation : The full path to the PDF file to open. This should be relative from where the julia command has been run (not relative to this source file)\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.TxtReader.appendToFile-Tuple{String, String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.TxtReader.appendToFile","text":"Append the given contents into a file specified at filename. A new file will be created if filename doesn't already exist.\n\nNOTE: No ' ' newline character will be appended. It is the  caller's responsibility to decide if the contents should have a ' ' newline character or not. \n\nParameters\n\nfilename: String     The name of the file to open. Relative file paths are evaluated from     the directory where the julia command was run. Typically the root level     of the project contents: String     The exact text to append into the file.\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.TxtReader.getAllTextInFile-Tuple{String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.TxtReader.getAllTextInFile","text":"Open the provided filename, load all the data into memory, and return. This function will also manage the file socket open(...) close(...) properly. If there was an error in opening or reading the file then the empty  string will be returned\n\nParameters\n\nfilename: String     The name of the file to open. Relative file paths are evaluated from     the directory where the julia command was run. Typically the root level     of the project\n\nReturns: String     The entire contets of the file, or an empty string if there was an issue\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.TxtReader.splitFileIntoParts-Tuple{String, String, Int64}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.TxtReader.splitFileIntoParts","text":"A simple script that allows a user to split a large file into multiple smaller files.  This will create splits # of children files, each with a file size ~1/splits  of the origional target file.\n\nParameters\n\nfileToSplit : String     The name of the file to read and split into multiple parts.     If an absolute file path is given then that will be     used. Otherwise, relative file paths are evaluated from the location that     the julia command was run from (typically the root level of this project) outputFileNameBase : String     The template for the name of the children split-out files.      Each split out file with have the format of <outputFileNameBase>_<#>      where # starts at 1 and increments by 1 for each subsequent file.      There will be splits number of children files splits : Int     How many children files should be created?\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Embedding.embed-Tuple{Embedder, String}","page":"Home","title":"Juissie.SemanticSearch.Embedding.embed","text":"function embed(embedder::Embedder, text::String)::AbstractVector\n\nEmbeds a textual sequence using a provided model\n\nParameters\n\nembedder : Embedder     an initialized Embedder struct text : String     the text sequence you want to embed\n\nNotes\n\nThis is sort of like a class method for the Embedder\n\nJulia has something called multiple dispatch that can be used to  make this cleaner, but I'm going to handle that at a later times\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Embedding.embed_from_bert-Tuple{Embedder, String}","page":"Home","title":"Juissie.SemanticSearch.Embedding.embed_from_bert","text":"function embed_from_bert(embedder::Embedder, text::String)\n\nEmbeds a textual sequence using a provided Bert model\n\nParameters\n\nembedder : Embedder     an initialized Embedder struct     the associated model and tokenizer should be Bert-specific text : String     the text sequence you want to embed\n\nreturn : cls_embedding     The results from passing the text through the encoder, throught the model,     and after stripping \n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Embedding.Embedder","page":"Home","title":"Juissie.SemanticSearch.Embedding.Embedder","text":"struct Embedder\n\nA struct for holding a model and a tokenizer\n\nAttributes\n\ntokenizer : a tokenizer object, e.g. BertTextEncoder     maps your string to tokens the model can understand model : a model object, e.g. HGFBertModel     the actual model architecture and weights to perform inference with\n\nNotes\n\nYou can get class-like behavior in Julia by defining a struct and functions that operate on that struct.\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.SemanticSearch.Embedding.Embedder-Tuple{String}","page":"Home","title":"Juissie.SemanticSearch.Embedding.Embedder","text":"function Embedder(model_name::String)\n\nFunction to initialize an Embedder struct from a HuggingFace model path.\n\nParameters\n\nmodel_name : String     a path to a HuggingFace-hosted model     e.g. \"BAAI/bge-small-en-v1.5\"\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.TextUtils.chunkify","page":"Home","title":"Juissie.SemanticSearch.TextUtils.chunkify","text":"function chunkify(text::String, tokenizer, sequence_length::Int=512)\n\nSplits a provided text (e.g. paragraph) into chunks that are each as many sentences as possible while keeping the chunk's token lenght below the sequence_length. This ensures that each chunk can be fully encoded by the embedder.\n\nParameters\n\ntext : String     The text you want to split into chunks. tokenizer : a tokenizer object, e.g. BertTextEncoder     The tokenizer you will be using sequence_length : Int     The maximum number of tokens per chunk.     Ideally, should correspond to the max sequence length of the tokenizer\n\nExample Usage\n\n>>> chunkify(\n    '''Hold me closer, tiny dancer. Count the headlights on the highway. Lay me down in sheets of linen. Peter Piper picked a peck of pickled peppers. A peck of pickled peppers Peter Piper picked.\n    ''', \n    corpus.embedder.tokenizer, \n    20\n)\n\n4-element Vector{Any}:\n\"Hold me closer, tiny dancer. Count the headlights on the highway.\"\n\"Lay me down in sheets of linen.\"\n\"Peter Piper picked a peck of pickled peppers.\"\n\"A peck of pickled peppers Peter Piper picked.\"\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.SemanticSearch.TextUtils.get_files_path-Tuple{}","page":"Home","title":"Juissie.SemanticSearch.TextUtils.get_files_path","text":"function get_files_path()\n\nSimple function to return the path to the files subdirectory.\n\nExample Usage\n\ntestbinpath = getfilespath()*\"test.bin\"\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.TextUtils.read_html_url","page":"Home","title":"Juissie.SemanticSearch.TextUtils.read_html_url","text":"read_html_url(url::String, elements::Array{String})\n\nReturns a string of text from the provided HTML elements on a webpage.\n\nParameters\n\nurl : String     the url you want to read elements : Array{String}     html elements to look for in the web page, e.g. [\"h1\", \"p\"].\n\nNotes\n\nDefaults to extracting headers and paragraphs\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.SemanticSearch.TextUtils.sentence_splitter-Tuple{String}","page":"Home","title":"Juissie.SemanticSearch.TextUtils.sentence_splitter","text":"function sentence_splitter(text::String)\n\nUses basic regex to divide a provided text (e.g. paragraph) into sentences.\n\nParameters\n\ntext : String     The text you want to split into sentences.\n\nNotes\n\nRegex is hard to read. The first part looks for spaces following  end-of-sentence punctuation. The second part matches at the end of the string.\n\nRegex in Julia uses an r identifier prefix.\n\nReferences\n\nhttps://www.geeksforgeeks.org/regular-expressions-in-julia/\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.PdfReader.bufferToString!-Tuple{IOBuffer}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.PdfReader.bufferToString!","text":"Extract the contents of the buffer and convert it into a string object\n\nWARNING: This function will clear out the contents of the buffer\n\nParameters\n\nbuff : The buffer to clear, it's contents will be returned as a string\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.PdfReader.getAllTextInPDF","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.PdfReader.getAllTextInPDF","text":"Extract all the text data from the provided pdf file.\n\nOpen the pdf at the provided file location, extract all the text data from it (as far as possible), and return that text data as a vector of strings. Each entry in the rsult vector is the appended sum of some number of pages in the PDF. 100 Pages per entry is default. For example, the getAllTextInPDF(...)[0] will be a  long string containing 100 pages worth of data. The next entry represents the next 100  pages, etc.\n\nNOTE: This function is a \"best effort\" function, meaning that it will try to extract as many pages as it can. But if there are pages that are invalid, or otherwise can not be properly parsed then they will simply be ignored and not included in the  returned strings.\n\nParameters\n\nfileLocation : The full path to the PDF file to open. This should be relative from where the julia command has been run (not relative to this source file)\n\npagesPerEntry : How many pages should be collected into the buffere before turning it into an entry in the result vector.\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.SemanticSearch.Backend.PdfReader.getPagesFromPdf-Tuple{PDFIO.PD.PDDocImpl, Number, Number}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.PdfReader.getPagesFromPdf","text":"Collect and return all the text data found in the pdf file found in the provided page range.\n\nUsing the provided PDF Handel, loop over all the pages in the range and attempt to extract the text data. All the collected data will be returned.\n\nThe specific pages to read are defined by [firstPageInclusive, lastPageInclusive] which (naturally) defines an inclusive range. Meaning the first and last page number will be included in the returned string. These ranges SHOULD be valid (ie, in the range [1, MaxPageCount]) but error checking will coerce the values to a proper range.\n\nParameters\n\npdfHandel : The PDF file to extract data from firstPageInclusive : The first page in the range to read lastPageInclusive : The last page in the range to read\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.PdfReader.getPagesFromPdf-Tuple{String, Number, Number}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.PdfReader.getPagesFromPdf","text":"Collect and return all the text data found in the pdf file found in the provided page range.\n\nUsing the provided file path, open the PDF and loop over all the pages in the range and attempt to extract the text data. All the collected data will be returned.\n\nThe specific pages to read are defined by [firstPageInclusive, lastPageInclusive] which (naturally) defines an inclusive range. Meaning the first and last page number will be included in the returned string. These ranges SHOULD be valid (ie, in the range [1, MaxPageCount]) but error checking will coerce the values to a proper range.\n\nParameters\n\nfileLocation : The location of the PDF to read firstPageInclusive : The first page in the range to read lastPageInclusive : The last page in the range to read\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.PdfReader.getPagesInPDF_All-Tuple{String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.PdfReader.getPagesInPDF_All","text":"Extract all the text data from the provided pdf file.\n\nOpen the pdf at the provided file location, extract all the text data from it (as far as possible), and return that text data as a vector of strings. Each entry in the result vector is the data from a single page of the PDF file.\n\nNOTE: This function is a \"best effort\" function, meaning that it will try to extract as many pages as it can. But if there are pages that are invalid, or otherwise can not be properly parsed then they will simply be ignored and not included in the  returned strings.\n\nParameters\n\nfileLocation : The full path to the PDF file to open. This should be relative from where the julia command has been run (not relative to this source file)\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.Embedding.embed-Tuple{Juissie.SemanticSearch.Backend.Embedding.Embedder, String}","page":"Home","title":"Juissie.SemanticSearch.Backend.Embedding.embed","text":"function embed(embedder::Embedder, text::String)::AbstractVector\n\nEmbeds a textual sequence using a provided model\n\nParameters\n\nembedder : Embedder     an initialized Embedder struct text : String     the text sequence you want to embed\n\nNotes\n\nThis is sort of like a class method for the Embedder\n\nJulia has something called multiple dispatch that can be used to  make this cleaner, but I'm going to handle that at a later times\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.Embedding.embed_from_bert-Tuple{Juissie.SemanticSearch.Backend.Embedding.Embedder, String}","page":"Home","title":"Juissie.SemanticSearch.Backend.Embedding.embed_from_bert","text":"function embed_from_bert(embedder::Embedder, text::String)\n\nEmbeds a textual sequence using a provided Bert model\n\nParameters\n\nembedder : Embedder     an initialized Embedder struct     the associated model and tokenizer should be Bert-specific text : String     the text sequence you want to embed\n\nreturn : cls_embedding     The results from passing the text through the encoder, throught the model,     and after stripping \n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.Embedding.Embedder","page":"Home","title":"Juissie.SemanticSearch.Backend.Embedding.Embedder","text":"struct Embedder\n\nA struct for holding a model and a tokenizer\n\nAttributes\n\ntokenizer : a tokenizer object, e.g. BertTextEncoder     maps your string to tokens the model can understand model : a model object, e.g. HGFBertModel     the actual model architecture and weights to perform inference with\n\nNotes\n\nYou can get class-like behavior in Julia by defining a struct and functions that operate on that struct.\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.SemanticSearch.Backend.Embedding.Embedder-Tuple{String}","page":"Home","title":"Juissie.SemanticSearch.Backend.Embedding.Embedder","text":"function Embedder(model_name::String)\n\nFunction to initialize an Embedder struct from a HuggingFace model path.\n\nParameters\n\nmodel_name : String     a path to a HuggingFace-hosted model     e.g. \"BAAI/bge-small-en-v1.5\"\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.TextUtils.chunkify","page":"Home","title":"Juissie.SemanticSearch.Backend.TextUtils.chunkify","text":"function chunkify(text::String, tokenizer, sequence_length::Int=512)\n\nSplits a provided text (e.g. paragraph) into chunks that are each as many sentences as possible while keeping the chunk's token lenght below the sequence_length. This ensures that each chunk can be fully encoded by the embedder.\n\nParameters\n\ntext : String     The text you want to split into chunks. tokenizer : a tokenizer object, e.g. BertTextEncoder     The tokenizer you will be using sequence_length : Int     The maximum number of tokens per chunk.     Ideally, should correspond to the max sequence length of the tokenizer\n\nExample Usage\n\n>>> chunkify(\n    '''Hold me closer, tiny dancer. Count the headlights on the highway. Lay me down in sheets of linen. Peter Piper picked a peck of pickled peppers. A peck of pickled peppers Peter Piper picked.\n    ''', \n    corpus.embedder.tokenizer, \n    20\n)\n\n4-element Vector{Any}:\n\"Hold me closer, tiny dancer. Count the headlights on the highway.\"\n\"Lay me down in sheets of linen.\"\n\"Peter Piper picked a peck of pickled peppers.\"\n\"A peck of pickled peppers Peter Piper picked.\"\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.SemanticSearch.Backend.TextUtils.get_files_path-Tuple{}","page":"Home","title":"Juissie.SemanticSearch.Backend.TextUtils.get_files_path","text":"function get_files_path()\n\nSimple function to return the path to the files subdirectory.\n\nExample Usage\n\ntestbinpath = getfilespath()*\"test.bin\"\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.SemanticSearch.Backend.TextUtils.read_html_url","page":"Home","title":"Juissie.SemanticSearch.Backend.TextUtils.read_html_url","text":"read_html_url(url::String, elements::Array{String})\n\nReturns a string of text from the provided HTML elements on a webpage.\n\nParameters\n\nurl : String     the url you want to read elements : Array{String}     html elements to look for in the web page, e.g. [\"h1\", \"p\"].\n\nNotes\n\nDefaults to extracting headers and paragraphs\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.SemanticSearch.Backend.TextUtils.sentence_splitter-Tuple{String}","page":"Home","title":"Juissie.SemanticSearch.Backend.TextUtils.sentence_splitter","text":"function sentence_splitter(text::String)\n\nUses basic regex to divide a provided text (e.g. paragraph) into sentences.\n\nParameters\n\ntext : String     The text you want to split into sentences.\n\nNotes\n\nRegex is hard to read. The first part looks for spaces following  end-of-sentence punctuation. The second part matches at the end of the string.\n\nRegex in Julia uses an r identifier prefix.\n\nReferences\n\nhttps://www.geeksforgeeks.org/regular-expressions-in-julia/\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.Embedding.embed-Tuple{Juissie.Generation.SemanticSearch.Backend.Embedding.Embedder, String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.Embedding.embed","text":"function embed(embedder::Embedder, text::String)::AbstractVector\n\nEmbeds a textual sequence using a provided model\n\nParameters\n\nembedder : Embedder     an initialized Embedder struct text : String     the text sequence you want to embed\n\nNotes\n\nThis is sort of like a class method for the Embedder\n\nJulia has something called multiple dispatch that can be used to  make this cleaner, but I'm going to handle that at a later times\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.Embedding.embed_from_bert-Tuple{Juissie.Generation.SemanticSearch.Backend.Embedding.Embedder, String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.Embedding.embed_from_bert","text":"function embed_from_bert(embedder::Embedder, text::String)\n\nEmbeds a textual sequence using a provided Bert model\n\nParameters\n\nembedder : Embedder     an initialized Embedder struct     the associated model and tokenizer should be Bert-specific text : String     the text sequence you want to embed\n\nreturn : cls_embedding     The results from passing the text through the encoder, throught the model,     and after stripping \n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.Backend.Embedding.Embedder","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.Embedding.Embedder","text":"struct Embedder\n\nA struct for holding a model and a tokenizer\n\nAttributes\n\ntokenizer : a tokenizer object, e.g. BertTextEncoder     maps your string to tokens the model can understand model : a model object, e.g. HGFBertModel     the actual model architecture and weights to perform inference with\n\nNotes\n\nYou can get class-like behavior in Julia by defining a struct and functions that operate on that struct.\n\n\n\n\n\n","category":"type"},{"location":"#Juissie.Generation.SemanticSearch.Backend.Embedding.Embedder-Tuple{String}","page":"Home","title":"Juissie.Generation.SemanticSearch.Backend.Embedding.Embedder","text":"function Embedder(model_name::String)\n\nFunction to initialize an Embedder struct from a HuggingFace model path.\n\nParameters\n\nmodel_name : String     a path to a HuggingFace-hosted model     e.g. \"BAAI/bge-small-en-v1.5\"\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.TextUtils.chunkify","page":"Home","title":"Juissie.Generation.SemanticSearch.TextUtils.chunkify","text":"function chunkify(text::String, tokenizer, sequence_length::Int=512)\n\nSplits a provided text (e.g. paragraph) into chunks that are each as many sentences as possible while keeping the chunk's token lenght below the sequence_length. This ensures that each chunk can be fully encoded by the embedder.\n\nParameters\n\ntext : String     The text you want to split into chunks. tokenizer : a tokenizer object, e.g. BertTextEncoder     The tokenizer you will be using sequence_length : Int     The maximum number of tokens per chunk.     Ideally, should correspond to the max sequence length of the tokenizer\n\nExample Usage\n\n>>> chunkify(\n    '''Hold me closer, tiny dancer. Count the headlights on the highway. Lay me down in sheets of linen. Peter Piper picked a peck of pickled peppers. A peck of pickled peppers Peter Piper picked.\n    ''', \n    corpus.embedder.tokenizer, \n    20\n)\n\n4-element Vector{Any}:\n\"Hold me closer, tiny dancer. Count the headlights on the highway.\"\n\"Lay me down in sheets of linen.\"\n\"Peter Piper picked a peck of pickled peppers.\"\n\"A peck of pickled peppers Peter Piper picked.\"\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.SemanticSearch.TextUtils.get_files_path-Tuple{}","page":"Home","title":"Juissie.Generation.SemanticSearch.TextUtils.get_files_path","text":"function get_files_path()\n\nSimple function to return the path to the files subdirectory.\n\nExample Usage\n\ntestbinpath = getfilespath()*\"test.bin\"\n\n\n\n\n\n","category":"method"},{"location":"#Juissie.Generation.SemanticSearch.TextUtils.read_html_url","page":"Home","title":"Juissie.Generation.SemanticSearch.TextUtils.read_html_url","text":"read_html_url(url::String, elements::Array{String})\n\nReturns a string of text from the provided HTML elements on a webpage.\n\nParameters\n\nurl : String     the url you want to read elements : Array{String}     html elements to look for in the web page, e.g. [\"h1\", \"p\"].\n\nNotes\n\nDefaults to extracting headers and paragraphs\n\n\n\n\n\n","category":"function"},{"location":"#Juissie.Generation.SemanticSearch.TextUtils.sentence_splitter-Tuple{String}","page":"Home","title":"Juissie.Generation.SemanticSearch.TextUtils.sentence_splitter","text":"function sentence_splitter(text::String)\n\nUses basic regex to divide a provided text (e.g. paragraph) into sentences.\n\nParameters\n\ntext : String     The text you want to split into sentences.\n\nNotes\n\nRegex is hard to read. The first part looks for spaces following  end-of-sentence punctuation. The second part matches at the end of the string.\n\nRegex in Julia uses an r identifier prefix.\n\nReferences\n\nhttps://www.geeksforgeeks.org/regular-expressions-in-julia/\n\n\n\n\n\n","category":"method"},{"location":"#Generation.SemanticSearch.Backend.index-Tuple{Generation.SemanticSearch.Backend.Corpus}","page":"Home","title":"Generation.SemanticSearch.Backend.index","text":"function index(corpus::Corpus)\n\nConstructs the HNSW vector index from the data available. If the corpus has a corpus_name, then we also save the new index to disk. Must be run before searching.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use\n\n\n\n\n\n","category":"method"},{"location":"#Generation.SemanticSearch.Backend.load_corpus-Tuple{String}","page":"Home","title":"Generation.SemanticSearch.Backend.load_corpus","text":"function load_corpus(corpus_name)\n\nLoads an already-initialized corpus from its associated \"artifacts\" (relational database, vector index, and informational json).\n\nParameters\n\ncorpus_name : str     the name of your EXISTING vector database\n\n\n\n\n\n","category":"method"},{"location":"#Generation.SemanticSearch.Backend.search","page":"Home","title":"Generation.SemanticSearch.Backend.search","text":"function search(corpus::Corpus, query::String, k::Int=5)\n\nPerforms approximate nearest neighbor search to find the items in the vector index closest to the query.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use query : str     The text you want to search, e.g. your question     We embed this and perform semantic retrieval against the vector db k : int     The number of nearest-neighbor vectors to fetch\n\n\n\n\n\n","category":"function"},{"location":"#Generation.SemanticSearch.Backend.upsert_chunk-Tuple{Generation.SemanticSearch.Backend.Corpus, String, String}","page":"Home","title":"Generation.SemanticSearch.Backend.upsert_chunk","text":"function upsert_chunk(corpus::Corpus, chunk::String, doc_name::String)\n\nGiven a new chunk of text, get embedding and insert into our vector DB. Not actually a full upsert, because we have to reindex later. Process:\n\nGenerate an embedding for the text\nInsert metadata into database\nIncrement idx counter\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use chunk : str     This is the text content of the chunk you want to upsert docname : str     The name of the document that chunk is from. For instance, if you      were upserting all the chunks in an academic paper, docname might     be the name of that paper\n\nNotes\n\nIf the vectors have been indexed, this de-indexes them (i.e., they need to be indexed again). Currently, we handle this by setting hnsw to  nothing so that it gets caught later in search.\n\n\n\n\n\n","category":"method"},{"location":"#Generation.SemanticSearch.Backend.upsert_document-Tuple{Generation.SemanticSearch.Backend.Corpus, String, String}","page":"Home","title":"Generation.SemanticSearch.Backend.upsert_document","text":"function upsert_document(corpus::Corpus, doc_text::String, doc_name::String)\n\nUpsert a whole document (i.e., long string). Does so by splitting the document into appropriately-sized chunks so no chunk exceeds the embedder's tokenization max sequence length, while prioritizing sentence endings.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use doctext : str     A long string you want to upsert. We will break this into chunks and     upsert each chunk. docname : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Generation.SemanticSearch.Backend.upsert_document-Tuple{Generation.SemanticSearch.Backend.Corpus, Vector{String}, String}","page":"Home","title":"Generation.SemanticSearch.Backend.upsert_document","text":"function upsert_document(corpus::Corpus, documents::Vector{String}, doc_name::String)\n\nUpsert a collection of documents (i.e., a vector of long strings). Does so by upserting each entry of the provided documents vector (which in turn will chunkify, each document further into appropriately sized chunks).\n\nSee the upsert_document(...) above for more details\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use documents : Vector{String}     a collection of long strings to upsert. doc_name : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Generation.SemanticSearch.Backend.upsert_document_from_pdf-Tuple{Generation.SemanticSearch.Backend.Corpus, String, String}","page":"Home","title":"Generation.SemanticSearch.Backend.upsert_document_from_pdf","text":"function upsert_document_from_pdf(corpus::Corpus, filePath::String, doc_name::String)\n\nUpsert all the data in a PDF file into the provided corpus. See the upsert_document(...) above for more details.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use filePath : String     The path to the PDF file to read doc_name : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Generation.SemanticSearch.Backend.upsert_document_from_txt-Tuple{Generation.SemanticSearch.Backend.Corpus, String, String}","page":"Home","title":"Generation.SemanticSearch.Backend.upsert_document_from_txt","text":"function upsert_document_from_txt(corpus::Corpus, filePath::String, doc_name::String)\n\nUpsert all the data from the text file into the provided corpus.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use filePath : String     The path to the txt file to read doc_name : str     The name of the document the content is from\n\n\n\n\n\n","category":"method"},{"location":"#Generation.SemanticSearch.Backend.upsert_document_from_url","page":"Home","title":"Generation.SemanticSearch.Backend.upsert_document_from_url","text":"function upsert_document_from_url(corpus::Corpus, url::String, doc_name::String, elements::Array{String}=[\"h1\", \"h2\", \"p\"])\n\nExtracts element-tagged text from HTML and upserts as a document.\n\nParameters\n\ncorpus : an initialized Corpus object     the corpus / \"vector database\" you want to use url : String     The url you want to scrape for text doc_name : str     The name of the document the content is from elements : Array{String}     A list of HTML elements you want to pull the text from\n\n\n\n\n\n","category":"function"},{"location":"#Generation.SemanticSearch.Backend.Corpus","page":"Home","title":"Generation.SemanticSearch.Backend.Corpus","text":"struct Corpus\n\nBasically a vector database. It will have these attributes:\n\na relational database (SQLite)\na vector index (HNSW)\nan embedder (via Embedding.jl)\n\nAttributes\n\ncorpusname : String or Nothing     this is the name of your corpus and will be used to access saved          corpuses     if Nothing, we can't save/load and everything will be in-memory db : a SQLite.DB connection object     this is a real relational database to store metadata (e.g. chunk text, doc name) hnsw : Hierarchical Navigable Small World object     this is our searchable vector index embedder : Embedder     an initialized Embedder struct maxseqlen : int     The maximum number of tokens per chunk.     This should be the max sequence length of the tokenizer data : Vector{Any}     The embeddings get stored here before we create the vector index nextidx : int     stores the index we'll use for the next-upserted chunk\n\nNotes\n\nThe struct is mutable because we want to be able to change things like incrementing next_idx.\n\n\n\n\n\n","category":"type"},{"location":"#Generation.SemanticSearch.Backend.Corpus-2","page":"Home","title":"Generation.SemanticSearch.Backend.Corpus","text":"function Corpus(corpus_name::String, embedder_model_path::String=\"BAAI/bge-small-en-v1.5\")\n\nInitializes a Corpus struct.\n\nIn particular, does the following:\n\nInitializes an embedder object\nCreates a SQLite databse with the corpus name. It should have:\n\nrow-wise primary key uuid\ndoc_name representing the parent document\nchunk text \n\nWe can add more metadata later, if desired\n\nParameters\n\ncorpusname : str or nothing     the name that you want to give the database     optional. if left as nothing, we use an in-memory database embeddermodelpath : str     a path to a HuggingFace-hosted model     e.g. \"BAAI/bge-small-en-v1.5\" maxseq_len : int     The maximum number of tokens per chunk.     This should be the max sequence length of the tokenizer\n\n\n\n\n\n","category":"type"},{"location":"#Generation.build_full_query","page":"Home","title":"Generation.build_full_query","text":"function build_full_query(query::String, context::OptionalContext=nothing)\n\nGiven a query and a list of contextual chunks, construct a full query incorporating both.\n\nParameters\n\nquery : String     the main instruction or query string context : OptionalContext, which is Union{Vector{String}, Nothing}     optional list of chunks providing additional context for the query\n\nNotes\n\nWe base our prompt off the Alpaca prompt, found here: https://github.com/tatsu-lab/stanford_alpaca with minor modifications that reflect our response preferences.\n\n\n\n\n\n","category":"function"},{"location":"#Generation.check_oai_key_format-Tuple{String}","page":"Home","title":"Generation.check_oai_key_format","text":"function check_oai_key_format(key::String)\n\nUses regex to check if a provided string is in the expected format of an OpenAI     API Key\n\nParameters\n\nkey : String     the key you want to check\n\nNotes\n\nSee here for more on the regex:\n\nhttps://en.wikibooks.org/wiki/IntroducingJulia/Stringsandcharacters#Findingandreplacingthingsinsidestrings\n\nUses format rule provided here:\n\nhttps://github.com/secretlint/secretlint/issues/676\nhttps://community.openai.com/t/what-are-the-valid-characters-for-the-apikey/288643\n\nNote that this only checks the key format, not whether the key is valid or has not  been revoked.\n\n\n\n\n\n","category":"method"},{"location":"#Generation.generate","page":"Home","title":"Generation.generate","text":"generate(generator::Union{OAIGenerator, Nothing}, query::String, context::OptionalContext=nothing, temperature::Float64=0.7)\n\nGenerate a response based on a given query and optional context using the specified OAIGenerator. This function constructs a full query, sends it to the OpenAI API, and returns the generated response.\n\nParameters\n\ngenerator : Union{OAIGenerator, Nothing}     an initialized generator (e..g OAIGenerator)     leaving this as a union with nothing to note that we may want to support other      generator types in the future (e.g. HFGenerator, etc.) query : String     the main query string. This is basically your question context : OptionalContext, which is Union{Vector{String}, Nothing}     optional list of contextual chunk strings to provide the generator additional      context for the query. Ultimately, these will be coming from our vector DB temperature : Float64     controls the stochasticity of the output generated by the model\n\n\n\n\n\n","category":"function"},{"location":"#Generation.generate_with_corpus","page":"Home","title":"Generation.generate_with_corpus","text":"function generate_with_corpus(generator::Union{OAIGenerator, Nothing}, corpus::Corpus, query::String, k::Int=5, temperature::Float64=0.7)\n\nParameters\n\ngenerator : Union{OAIGenerator, Nothing}     an initialized generator (e..g OAIGenerator)     leaving this as a union with nothing to note that we may want to support other      generator types in the future (e.g. HFGenerator, etc.) corpus : an initialized Corpus object     the corpus / \"vector database\" you want to use query : String     the main instruction or query string. This is basically your question k : int     The number of nearest-neighbor vectors to fetch from the corpus to build your context temperature : Float64     controls the stochasticity of the output generated by the model\n\n\n\n\n\n","category":"function"},{"location":"#Generation.load_OAIGeneratorWithCorpus","page":"Home","title":"Generation.load_OAIGeneratorWithCorpus","text":"function load_OAIGeneratorWithCorpus(corpus_name::String, auth_token::Union{String, Nothing}=nothing)\n\nLoads an existing corpus and uses it to initialize an OAIGeneratorWithCorpus\n\nParameters\n\ncorpusname : str     the name that you want to give the database authtoken :: Union{String, Nothing}     this is your OPENAI API key. You can either pass it explicitly as a string     or leave this argument as nothing. In the latter case, we will look in your     environmental variables for \"OAI_KEY\"\n\nNotes\n\ncorpusname is ordered first because Julia uses positional arguments and  authtoken is optional.\n\nWhen instantiating a new OAIGenerator in an externally-viewable setting (e.g. notebooks committed to GitHub or a public demo), it is important to place a semicolon after the command, e.g.  '''generator=loadOAIGeneratorWithCorpus(\"greekphilosophers\");''' to ensure that your OAI API key is not inadvertently shared.\n\n\n\n\n\n","category":"function"},{"location":"#Generation.load_OllamaGeneratorWithCorpus","page":"Home","title":"Generation.load_OllamaGeneratorWithCorpus","text":"function load_OllamaGeneratorWithCorpus(corpus_name::String, model_name::String = \"mistral:7b-instruct\")\n\nLoads an existing corpus and uses it to initialize an OllamaGeneratorWithCorpus\n\nParameters\n\ncorpusname : str     the name that you want to give the database modelname :: String     this is an Ollama model tag. see https://ollama.com/library     defaults to mistral 7b instruct\n\nNotes\n\ncorpusname is ordered first because Julia uses positional arguments and  modelname is optional.\n\n\n\n\n\n","category":"function"},{"location":"#Generation.upsert_chunk_to_generator-Tuple{GeneratorWithCorpus, String, String}","page":"Home","title":"Generation.upsert_chunk_to_generator","text":"function upsert_chunk_to_generator(generator::GeneratorWithCorpus, chunk::String, doc_name::String)\n\nEquivalent to Backend.upsert_chunk, but takes a GeneratorWithCorpus instead of a Corpus.\n\nParameters\n\ngenerator : any struct that subtypes GeneratorWithCorpus     the generator (with corpus) you want to use chunk : str     This is the text content of the chunk you want to upsert docname : str     The name of the document that chunk is from. For instance, if you      were upserting all the chunks in an academic paper, docname might     be the name of that paper\n\nNotes\n\nOne would expect Julia's multiple dispatch to allow us to call this upsertchunk, but not so. The conflict arises in Juissie, where  we would have both SemanticSearch and Generation exporting  upsertchunk. This means any uses of it in Juissie must be  qualified, and without doing so, neither actually gets defined.\n\n\n\n\n\n","category":"method"},{"location":"#Generation.upsert_document_from_url_to_generator","page":"Home","title":"Generation.upsert_document_from_url_to_generator","text":"function upsert_document_from_url_to_generator(generator::GeneratorWithCorpus, url::String, doc_name::String, elements::Array{String}=[\"h1\", \"h2\", \"p\"])\n\nEquivalent to Backend.upsertdocumentfrom_url, but takes a  GeneratorWithCorpus instead of a Corpus.\n\nParameters\n\ngenerator : any struct that subtypes GeneratorWithCorpus     the generator (with corpus) you want to use url : String     The url you want to scrape for text doc_name : str     The name of the document the content is from elements : Array{String}     A list of HTML elements you want to pull the text from\n\nNotes\n\nSee note for upsertchunkto_generator - same idea.\n\n\n\n\n\n","category":"function"},{"location":"#Generation.upsert_document_to_generator-Tuple{GeneratorWithCorpus, String, String}","page":"Home","title":"Generation.upsert_document_to_generator","text":"function upsert_document_to_generator(generator::GeneratorWithCorpus, doc_text::String, doc_name::String)\n\nEquivalent to Backend.upsert_document, but takes a GeneratorWithCorpus instead of a Corpus.\n\nParameters\n\ngenerator : any struct that subtypes GeneratorWithCorpus     the generator (with corpus) you want to use doctext : str     A long string you want to upsert. We will break this into chunks and     upsert each chunk. docname : str     The name of the document the content is from\n\nNotes\n\nSee note for upsertchunkto_generator - same idea.\n\n\n\n\n\n","category":"method"},{"location":"#Generation.OAIGenerator","page":"Home","title":"Generation.OAIGenerator","text":"function OAIGenerator(auth_token::Union{String, Nothing})\n\nInitializes an OAIGenerator struct.\n\nParameters\n\nauthtoken :: Union{String, Nothing}     this is your OPENAI API key. You can either pass it explicitly as a string     or leave this argument as nothing. In the latter case, we will look in your     environmental variables for \"OAIKEY\"\n\nNotes\n\nWhen instantiating a new OAIGenerator in an externally-viewable setting (e.g. notebooks committed to GitHub or a public demo), it is important to place a semicolon after the command, e.g.  '''generator=loadOAIGeneratorWithCorpus(\"greekphilosophers\");''' to ensure that your OAI API key is not inadvertently shared.\n\n\n\n\n\n","category":"type"},{"location":"#Generation.OAIGenerator-2","page":"Home","title":"Generation.OAIGenerator","text":"struct OAIGenerator\n\nA struct for handling natural language generation via OpenAI's gpt-3.5-turbo completion endpoint.\n\nAttributes\n\nurl : String     the URL of the OpenAI API endpoint header : Vector{Pair{String, String}}     key-value pairs representing the HTTP headers for the request body : Dict{String, Any}     this is the JSON payload to be sent in the body of the request\n\nNotes\n\nAll natural language generation should be done via a \"Generator\" object of some kind for consistency.\n\nWhen instantiating a new OAIGenerator in an externally-viewable setting (e.g. notebooks committed to GitHub or a public demo), it is important to place a semicolon after the command, e.g.  '''generator=loadOAIGeneratorWithCorpus(\"greekphilosophers\");''' to ensure that your OAI API key is not inadvertently shared.\n\n\n\n\n\n","category":"type"},{"location":"#Generation.OAIGeneratorWithCorpus","page":"Home","title":"Generation.OAIGeneratorWithCorpus","text":"function OAIGeneratorWithCorpus(auth_token::Union{String, Nothing}=nothing, corpus::Corpus)\n\nInitializes an OAIGeneratorWithCorpus.\n\nParameters\n\ncorpusname : str or nothing     the name that you want to give the database     optional. if left as nothing, we use an in-memory database authtoken :: Union{String, Nothing}     this is your OPENAI API key. You can either pass it explicitly as a string     or leave this argument as nothing. In the latter case, we will look in your     environmental variables for \"OAIKEY\" embeddermodelpath : str     a path to a HuggingFace-hosted model     e.g. \"BAAI/bge-small-en-v1.5\" maxseq_len : int     The maximum number of tokens per chunk.     This should be the max sequence length of the tokenizer\n\nNotes\n\nWhen instantiating a new OAIGenerator in an externally-viewable setting (e.g. notebooks committed to GitHub or a public demo), it is important to place a semicolon after the command, e.g.  '''generator=loadOAIGeneratorWithCorpus(\"greekphilosophers\");''' to ensure that your OAI API key is not inadvertently shared.\n\n\n\n\n\n","category":"type"},{"location":"#Generation.OAIGeneratorWithCorpus-2","page":"Home","title":"Generation.OAIGeneratorWithCorpus","text":"struct OAIGeneratorWithCorpus\n\nLike OAIGenerator, but has a corpus attached.\n\nAttributes\n\nurl : String     the URL of the OpenAI API endpoint header : Vector{Pair{String, String}}     key-value pairs representing the HTTP headers for the request body : Dict{String, Any}     this is the JSON payload to be sent in the body of the request corpus : an initialized Corpus object     the corpus / \"vector database\" you want to use\n\nNotes\n\nWhen instantiating a new OAIGenerator in an externally-viewable setting (e.g. notebooks committed to GitHub or a public demo), it is important to place a semicolon after the command, e.g.  '''generator=loadOAIGeneratorWithCorpus(\"greekphilosophers\");''' to ensure that your OAI API key is not inadvertently shared.\n\n\n\n\n\n","category":"type"},{"location":"#Generation.OllamaGenerator","page":"Home","title":"Generation.OllamaGenerator","text":"function OllamaGenerator(model_name::String = \"mistral:7b-instruct\")\n\nInitializes an OllamaGenerator struct for local text generation.\n\nParameters\n\nmodel_name :: String     this is an Ollama model tag. see https://ollama.com/library     defaults to mistral 7b instruct\n\n\n\n\n\n","category":"type"},{"location":"#Generation.OllamaGenerator-2","page":"Home","title":"Generation.OllamaGenerator","text":"struct OllamaGenerator\n\nA struct for handling natural language generation locally.\n\nAttributes\n\nurl : String     the URL of the local Ollama API endpoint header : Dict{String,Any}     HTTP header for the request body : Dict{String, Any}     this is the JSON payload to be sent in the body of the request\n\n\n\n\n\n","category":"type"},{"location":"#Generation.OllamaGeneratorWithCorpus","page":"Home","title":"Generation.OllamaGeneratorWithCorpus","text":"function OllamaGeneratorWithCorpus(corpus_name::Union{String,Nothing} = nothing, model_name::String = \"mistral:7b-instruct\", embedder_model_path::String = \"BAAI/bge-small-en-v1.5\", max_seq_len::Int = 512)\n\nInitializes an OllamaGeneratorWithCorpus.\n\nParameters\n\ncorpusname : str or nothing     the name that you want to give the database     optional. if left as nothing, we use an in-memory database modelname :: String     this is an Ollama model tag. see https://ollama.com/library     defaults to mistral 7b instruct embeddermodelpath : str     a path to a HuggingFace-hosted model     e.g. \"BAAI/bge-small-en-v1.5\" maxseqlen : int     The maximum number of tokens per chunk.     This should be the max sequence length of the tokenizer\n\n\n\n\n\n","category":"type"},{"location":"#Generation.OllamaGeneratorWithCorpus-2","page":"Home","title":"Generation.OllamaGeneratorWithCorpus","text":"struct_OllamaGeneratorWithCorpus\n\nLike OllamaGenerator, but has a corpus attached.\n\nAttributes\n\nurl : String     the URL of the local Ollama API endpoint header : Dict{String,Any}     HTTP header for the request body : Dict{String, Any}     this is the JSON payload to be sent in the body of the request corpus : an initialized Corpus object     the corpus / \"vector database\" you want to use\n\n\n\n\n\n","category":"type"},{"location":"#Generation.SemanticSearch.Backend.TextUtils.chunkify","page":"Home","title":"Generation.SemanticSearch.Backend.TextUtils.chunkify","text":"function chunkify(text::String, tokenizer, sequence_length::Int=512)\n\nSplits a provided text (e.g. paragraph) into chunks that are each as many sentences as possible while keeping the chunk's token lenght below the sequence_length. This ensures that each chunk can be fully encoded by the embedder.\n\nParameters\n\ntext : String     The text you want to split into chunks. tokenizer : a tokenizer object, e.g. BertTextEncoder     The tokenizer you will be using sequence_length : Int     The maximum number of tokens per chunk.     Ideally, should correspond to the max sequence length of the tokenizer\n\nExample Usage\n\n>>> chunkify(\n    '''Hold me closer, tiny dancer. Count the headlights on the highway. Lay me down in sheets of linen. Peter Piper picked a peck of pickled peppers. A peck of pickled peppers Peter Piper picked.\n    ''', \n    corpus.embedder.tokenizer, \n    20\n)\n\n4-element Vector{Any}:\n\"Hold me closer, tiny dancer. Count the headlights on the highway.\"\n\"Lay me down in sheets of linen.\"\n\"Peter Piper picked a peck of pickled peppers.\"\n\"A peck of pickled peppers Peter Piper picked.\"\n\n\n\n\n\n","category":"function"},{"location":"#Generation.SemanticSearch.Backend.TextUtils.get_files_path-Tuple{}","page":"Home","title":"Generation.SemanticSearch.Backend.TextUtils.get_files_path","text":"function get_files_path()\n\nSimple function to return the path to the files subdirectory.\n\nExample Usage\n\ntestbinpath = getfilespath()*\"test.bin\"\n\n\n\n\n\n","category":"method"},{"location":"#Generation.SemanticSearch.Backend.TextUtils.read_html_url","page":"Home","title":"Generation.SemanticSearch.Backend.TextUtils.read_html_url","text":"read_html_url(url::String, elements::Array{String})\n\nReturns a string of text from the provided HTML elements on a webpage.\n\nParameters\n\nurl : String     the url you want to read elements : Array{String}     html elements to look for in the web page, e.g. [\"h1\", \"p\"].\n\nNotes\n\nDefaults to extracting headers and paragraphs\n\n\n\n\n\n","category":"function"},{"location":"#Generation.SemanticSearch.Backend.TextUtils.sentence_splitter-Tuple{String}","page":"Home","title":"Generation.SemanticSearch.Backend.TextUtils.sentence_splitter","text":"function sentence_splitter(text::String)\n\nUses basic regex to divide a provided text (e.g. paragraph) into sentences.\n\nParameters\n\ntext : String     The text you want to split into sentences.\n\nNotes\n\nRegex is hard to read. The first part looks for spaces following  end-of-sentence punctuation. The second part matches at the end of the string.\n\nRegex in Julia uses an r identifier prefix.\n\nReferences\n\nhttps://www.geeksforgeeks.org/regular-expressions-in-julia/\n\n\n\n\n\n","category":"method"}]
}
