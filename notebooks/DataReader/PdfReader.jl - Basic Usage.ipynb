{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../../src/SemanticSearch/DataReader/PdfReader.jl\")\n",
    "\n",
    "pdfFilePath::String = \"../TestData/Massive Activations in Large Language Models.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pdf Reader Module\n",
    "\n",
    "This module is a useful wrapper for the PDFIO library https://github.com/sambitdash/PDFIO.jl?tab=readme-ov-file\n",
    "\n",
    "The PdfReader module allows a developer to quickly open and access the text data inside a pdf for upserting into a Corpus.\n",
    "\n",
    "### Third Party License - PDFIO \n",
    "PDFIO: https://github.com/sambitdash/PDFIO.jl?tab=readme-ov-file\n",
    "\n",
    "License for the PDFIO may be found here: https://github.com/sambitdash/PDFIO.jl?tab=License-1-ov-file#pdfio-license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PdfReader.getAllTextInPDF\n",
    "\n",
    "This function opens a target file, and returns a Vector of Strings. Each entry in this result represents 100 pages worth of text data from the source PDF. The content of the resultant vector are in order, IE the first entry is the first 100 pages, the next is pages 201-300 etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{String}:\n",
       " \"           to others, we name \"\u001b[93m\u001b[1m ⋯ 95251 bytes ⋯ \u001b[22m\u001b[39m\"                           30\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PdfReader.getAllTextInPDF(pdfFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PdfReader.getPagesInPDF_All\n",
    "\n",
    "This function opens a target file, and returns a Vector of Strings. Each entry in this result represents 1 single page worth of text data from the source PDF. The contents of the resultant vector are in correct page order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30-element Vector{String}:\n",
       " \"           to others, we name \"\u001b[93m\u001b[1m ⋯ 5120 bytes ⋯ \u001b[22m\u001b[39m\"                             2\"\n",
       " \"                              \"\u001b[93m\u001b[1m ⋯ 2679 bytes ⋯ \u001b[22m\u001b[39m\"                             3\"\n",
       " \"                      LLaMA2-7\"\u001b[93m\u001b[1m ⋯ 4298 bytes ⋯ \u001b[22m\u001b[39m\"                             4\"\n",
       " \"           Summary.We summariz\"\u001b[93m\u001b[1m ⋯ 3808 bytes ⋯ \u001b[22m\u001b[39m\"                             5\"\n",
       " \"                              \"\u001b[93m\u001b[1m ⋯ 4312 bytes ⋯ \u001b[22m\u001b[39m\"                             6\"\n",
       " \"           4 Effects on Attent\"\u001b[93m\u001b[1m ⋯ 4198 bytes ⋯ \u001b[22m\u001b[39m\"                             7\"\n",
       " \"           activations are onl\"\u001b[93m\u001b[1m ⋯ 4492 bytes ⋯ \u001b[22m\u001b[39m\"                             8\"\n",
       " \"               Value Updates  \"\u001b[93m\u001b[1m ⋯ 3543 bytes ⋯ \u001b[22m\u001b[39m\"                             9\"\n",
       " \"                   GPT-2 Defau\"\u001b[93m\u001b[1m ⋯ 3938 bytes ⋯ \u001b[22m\u001b[39m\"                            10\"\n",
       " \"               Patch Tokens   \"\u001b[93m\u001b[1m ⋯ 3374 bytes ⋯ \u001b[22m\u001b[39m\"                            11\"\n",
       " \"                Attention Dist\"\u001b[93m\u001b[1m ⋯ 4149 bytes ⋯ \u001b[22m\u001b[39m\"                            12\"\n",
       " \"           7 Conclusion\\n\\n     \"\u001b[93m\u001b[1m ⋯ 3588 bytes ⋯ \u001b[22m\u001b[39m\"                            13\"\n",
       " \"           Timothée Darcet, Ma\"\u001b[93m\u001b[1m ⋯ 3366 bytes ⋯ \u001b[22m\u001b[39m\"                            14\"\n",
       " ⋮\n",
       " \"           A.3 BOS Token<s>\\n  \"\u001b[93m\u001b[1m ⋯ 1445 bytes ⋯ \u001b[22m\u001b[39m\"                            20\"\n",
       " \"           A.4 Layer-Level Ana\"\u001b[93m\u001b[1m ⋯ 3702 bytes ⋯ \u001b[22m\u001b[39m\"                            21\"\n",
       " \"           B Additional Result\"\u001b[93m\u001b[1m ⋯ 4167 bytes ⋯ \u001b[22m\u001b[39m\"                            22\"\n",
       " \"                  Layer 1     \"\u001b[93m\u001b[1m ⋯ 3863 bytes ⋯ \u001b[22m\u001b[39m\"                            23\"\n",
       " \"             LLaMA2-7B, RMSNor\"\u001b[93m\u001b[1m ⋯ 3659 bytes ⋯ \u001b[22m\u001b[39m\"                            24\"\n",
       " \"           snow?\\\\n Check the f\"\u001b[93m\u001b[1m ⋯ 3991 bytes ⋯ \u001b[22m\u001b[39m\"                            25\"\n",
       " \"           Results.Regarding t\"\u001b[93m\u001b[1m ⋯ 3315 bytes ⋯ \u001b[22m\u001b[39m\"                            26\"\n",
       " \"           C.1 Massive Activat\"\u001b[93m\u001b[1m ⋯ 1254 bytes ⋯ \u001b[22m\u001b[39m\"                            27\"\n",
       " \"                              \"\u001b[93m\u001b[1m ⋯ 2776 bytes ⋯ \u001b[22m\u001b[39m\"                            28\"\n",
       " \"                   DINOv2 ViT-\"\u001b[93m\u001b[1m ⋯ 3284 bytes ⋯ \u001b[22m\u001b[39m\"                            29\"\n",
       " \"            Model family Model\"\u001b[93m\u001b[1m ⋯ 2160 bytes ⋯ \u001b[22m\u001b[39m\"                            30\"\n",
       " \"            Model family Model\"\u001b[93m\u001b[1m ⋯ 2160 bytes ⋯ \u001b[22m\u001b[39m\"                            30\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = PdfReader.getPagesInPDF_All(pdfFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           to others, we name themmassive activations. We demonstrate their presence in a wide range of LLMs,\n",
      "           spanning different model sizes and families.\n",
      "           We explore where massive activations are located in LLMs. Regarding the depth dimension of LLMs, the\n",
      "           appearance of massive activations is mostly abrupt: they emerge suddenly after a single layer of computation,\n",
      "           and diminish at the last few layers. Further, we ffnd massive activations occur in a small number of feature\n",
      "           dimensions that are input agnostic. Many of these activations are found within the starting word token and\n",
      "           delimiter tokens. Additionally, we show that massive activations are not the same as outlier features (Dettmers\n",
      "           et al.,2022), a previously known phenomenon in LLMs.\n",
      "           We show that massive activations act as ffxed but crucial bias terms in LLMs. Here by bias terms, we mean\n",
      "           certain internal states of the models that are independent from the inputs, analogous to the bias termbin a\n",
      "           linear layery=Wx+b. First, we show that massive activations play a critical role in LLMs’ capabilities. For\n",
      "           instance, in LLaMA2-7B, setting merely four massive activations (out of millions of activations) to zero would\n",
      "           result in catastrophic collapse in model performance. Further, setting them to their mean values does not\n",
      "           hurt the model, suggesting their role is equivalent to simple constant biases. Our analysis reveals that after\n",
      "           the initial layers, LLMs repurpose the tokens linked with massive activations to store these important biases.\n",
      "           Intriguingly, massive activations are closely connected with self-attention. In particular, we show massive\n",
      "           activations cause attention to be attracted to the tokens associated with them. Our ffndings extend the\n",
      "           observations from “attention sinks” (Xiao et al.,2023b)—we demonstrate that LLMs allocate excessive\n",
      "           attention to more than just the ffrst token, and provide an in-depth analysis on how such attention concentration\n",
      "           patterns arise. Our analysis suggests that LLMs try to learn implicit bias components in self-attention via\n",
      "           massive activations, during their pretraining phase. We thus experiment with augmenting self-attention with\n",
      "           additional key and value embeddings that are explicitly designed as biases. Remarkably, we demonstrate that\n",
      "           training with them eliminates the need for LLMs to learn massive activations.\n",
      "           Finally, we also observe massive activations in Vision Transformers (ViTs). They appear less frequently than\n",
      "           those in LLMs but are still in many of the ViTs we have examined. In these ViTs, they tend to appear at\n",
      "           ffxed feature dimensions, but notably at varying patch tokens. Moreover, we ffnd that these activations act\n",
      "           similarly as ffxed biases. Notably, we discuss the connections between massive activations and the recently\n",
      "           proposed “register tokens” in ViTs (Darcet et al.,2023). We show they both learn values independent of input\n",
      "           images, functioning as ffxed biases. This offers an alternative interpretation for register tokens than that in\n",
      "           the original work (Darcet et al.,2023), where they were hypothesized to aggregate global image information.\n",
      "\n",
      "\n",
      "           2 Massive Activations\n",
      "\n",
      "           We study autoregressive Transformers, which are built by a stack ofLdecoding layers. Each layerℓtakes\n",
      "           the previous hidden statehℓ−1 ∈RT×d as input and outputs a hidden statehℓ ∈RT×d .Tis the number of\n",
      "           tokens anddis the number of features. Transformer layers use residual connections (He et al.,2016), and the\n",
      "           computation can be formulated as:\n",
      "                                           hℓ =hℓ−1 +Fℓ (hℓ−1 )                             (1)\n",
      "           whereFℓ is the residual transformation. Note that this includes both attention and MLP blocks. Anactivation\n",
      "           denotes a speciffc scalar value in a hidden state. Unless otherwise speciffed, our study of activations is on the\n",
      "           hidden statehℓ , i.e., the output of residual summations, not any intermediate states insideFℓ .\n",
      "           Existence in LLMs.We start with an illustrative example on LLaMA2-7B. In Figure1, we visualize the\n",
      "           intermediate featureshℓ of interest. We feed this model with short sentences and visualize the activation\n",
      "           magnitudes (z-axis) of the hidden states at a middle layer.xandyaxes correspond to sequence and feature\n",
      "           dimensions respectively. Each blue row corresponds to the feature embedding of one token. We observe up to\n",
      "           four activations with signiffcantly large magnitudes. The largest activation (about 2,000) is approximately\n",
      "           10,000 times larger than the median magnitude (about 0.2). The sheer scale of these activations makes them\n",
      "           stand out from others. We thus refer to these special activations asmassive activations.\n",
      "\n",
      "\n",
      "                                                  2\n"
     ]
    }
   ],
   "source": [
    "println(result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this library with the Backend.jl module see this example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../../src/Juissie.jl\")\n",
    "\n",
    "corpus = Juissie.Corpus(\"test2\")\n",
    "Juissie.upsert_document_from_pdf(corpus, \n",
    "                            pdfFilePath, \n",
    "                            \"Massive Activations in Large Language Models\")\n",
    "\n",
    "idx_list, doc_names, chunks, distances = Juissie.search(\n",
    "    corpus, \n",
    "    \"Finding relevant layers\", \n",
    "    3\n",
    ")\n",
    "\n",
    "println(chunks[1])\n",
    "println(chunks[2])\n",
    "println(chunks[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
