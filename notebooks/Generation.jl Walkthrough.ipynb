{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c680e925-c250-45a2-a590-8c3f78ec727f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"../src/Juissie.jl\")\n",
    "using .Juissie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3bd96-b67c-49bd-b4bd-bdaff8e11f4a",
   "metadata": {},
   "source": [
    "### Initializing a Generator\n",
    "Generators are the main interface to our generator model. A Generator manages several stages of the \"Request LLM\" pipeline including:\n",
    "\n",
    "Generating the final query to send to the LLM. These queries will include: \n",
    " - The user's initial query\n",
    " - Contextual information from our vector DB related to the initial query\n",
    " - Other metadata\n",
    "\n",
    "A generator is also responsible for setting up the request object for the LLM. For example, a generator may need to: \n",
    " - create an HTTP request\n",
    " - manage access keys\n",
    " - translate raw text data into json\n",
    " - etc.\n",
    "\n",
    "The generator will make the request of the LLM. Usually an HTTP query to an externally hosted model, but it may be updated to query a locally hosted model too.\n",
    "\n",
    "The generator will also process the response from the LLM.\n",
    " - Extract response text from LLM\n",
    " - Trimming the response to a certain size\n",
    "\n",
    "In this example, we'll initialize an `OAIGenerator`, which calls OpenAI's\n",
    "gpt-3.5-turbo completion endpoint. This example requires an OpenAI API Key.\n",
    "\n",
    "If you have an OpenAI API key you can do one of two things:\n",
    "\n",
    "- Recommended: Leave the `auth_token` argument as `nothing`. When you do this, `OAIGenerator` will look in your environmental variables for a key called \"OAI_KEY\".\n",
    "\n",
    "- Not Recommended: Pass it directly to `OAIGenerator`, e.g. `generator = OAIGenerator(\"YOUR_KEY_HERE\")`. If you do this, though, make sure you don't accidentally commit your key to the git repo!\n",
    "\n",
    "\n",
    "We'll be doing the recommended option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed588e0-420c-4c31-bb55-a1816787c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = OAIGenerator();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5c5dd-f087-4437-a4f9-17446d291488",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate(\n",
    "    generator,\n",
    "    \"Count to 100 by increments of x\", # this is the main query, i.e. your question\n",
    "    [\"x=9\", \"Please don't include the number 45 in your counting\"] # this is the context we will provide (chunks from the vector db)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b5751",
   "metadata": {},
   "source": [
    "The above `generate(...)` function did several things under the hood:\n",
    " - It read the OpenAI API Key from the environmental variables.\n",
    " - Created a new meta-query that includes the \"main query\" and all the context items connected together\n",
    " - Use the new meta-query and API key to create an HTTP Rest Request targeting OpenAI's gtr-3.5-turbo model\n",
    " - Sent the HTTP request\n",
    " - Received the HTTP response\n",
    " - extracted the text from the response\n",
    "\n",
    "And finally, the extracted text was returned from the `generate(...)` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa402fa-8e64-4f3a-af3d-91d6ec1da7f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generating With A Corpus\n",
    "\n",
    "We're going to create a generator of type `OAIGeneratorWithCorpus`, which is just like `OAIGenerator`, except it also has a `Corpus` attached. The usual `upsert` functions one might apply to a `Corpus` have equivalents for structs that subtype `GeneratorWithCorpus`, such as:\n",
    "- `upsert_chunk_to_generator`\n",
    "- `upsert_document_to_generator`\n",
    "- `upsert_document_from_url_to_generator`\n",
    "\n",
    "See the `Backend.jl - Basic Usage.ipynb` Jupyter notebook for more details\n",
    "\n",
    "We'll fill the generator's `Corpus` with chunks from the Wikipedia article on Aristotle, then ask the generator a niche question whose answer is found in that article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd5d230-e251-4aaf-aa11-24070ce31c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Juissie.OAIGeneratorWithCorpus();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f7223-d553-452e-b012-b03d1f2c9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Juissie.upsert_document_from_url_to_generator(\n",
    "    generator, \n",
    "    \"https://en.wikipedia.org/wiki/Aristotle\", \n",
    "    \"Wikipedia: Aristotle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcc2a6",
   "metadata": {},
   "source": [
    "The `generate_with_corpus` function is similar to the `generate` function discussed above, except instead of the developer providing the context, the `Corpus` will search for relevant items similar to the user's initial query and those will be used as context. \n",
    "\n",
    "In the example below, the initial user query about the 6 elements of tragedy. The `Corpus` contained withing the generator will find the three most relevant chunks of data to this user query, and use those three chunks as context in addition to the query. \n",
    "\n",
    "We arbitrarily decided that the 3 most relevant chunks are enough context for this query. But this is adjustable, with the default being five. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298e132-9271-4dd8-97ea-efc46dad2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, idx_list, doc_names, chunks = Juissie.generate_with_corpus(\n",
    "    generator,\n",
    "    \"According to Aristotle, what are the six elements of which tragedy is composed?\",\n",
    "    3\n",
    ");\n",
    "\n",
    "println(result, idx_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343f49a-20ed-4812-bb60-13a3afe1a509",
   "metadata": {},
   "source": [
    "It just so happens that in this example, the top-retrieved context result is the exact section that has the answer (see the fourth to last sentence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff18de2-7837-4f57-957a-a327be74ad4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "println(chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8768c1a",
   "metadata": {},
   "source": [
    "In this way, with a sufficiently knowledgeable `Corpus`, we can reduce the amount of hallucinations from a LLM by providing relevant context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb888f-9b3d-416b-86a1-6c262323027a",
   "metadata": {},
   "source": [
    "### More Involved Example: Philosophers\n",
    "Here, we will create an on-disk corpus and fill it with the articles about Greek philosophers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6becafa8-038d-4001-8065-0c95b50e3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cur_msg(\"\")\n",
    "generator = Juissie.OAIGeneratorWithCorpus(\"greek_philosophers\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae4e34-9bb6-486a-aa55-d5e6c7a2001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_philosophers = Dict(\n",
    "    \"Wikipedia: Aristotle\" => \"https://en.wikipedia.org/wiki/Aristotle\",\n",
    "    \"Wikipedia: Democrates\" => \"https://en.wikipedia.org/wiki/Democrates\",\n",
    "    \"Wikipedia: Diogenes\" => \"https://en.wikipedia.org/wiki/Diogenes_Laertius\",\n",
    "    \"Wikipedia: Epictetus\" => \"https://en.wikipedia.org/wiki/Epictetus\",\n",
    "    \"Wikipedia: Epicurus\" => \"https://en.wikipedia.org/wiki/Epicurus\",\n",
    "    \"Wikipedia: Heraclitus\" => \"https://en.wikipedia.org/wiki/Heraclitus\",\n",
    "    \"Wikipedia: Parmenides\" => \"https://en.wikipedia.org/wiki/Parmenides\",\n",
    "    \"Wikipedia: Plato\" => \"https://en.wikipedia.org/wiki/Plato\",\n",
    "    \"Wikipedia: Socrates\" => \"https://en.wikipedia.org/wiki/Socrates\",\n",
    "    \"Wikipedia: Xenophon\" => \"https://en.wikipedia.org/wiki/Xenophon\",\n",
    "    \"Wikipedia: Ancient Greek philosophy\" => \"https://en.wikipedia.org/wiki/Ancient_Greek_philosophy\",\n",
    "    \"Internet Encyclopedia of Philosophy: Ancient Greek Philosophy\" => \"https://iep.utm.edu/ancient-greek-philosophy/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Presocratic Philosophy\" => \"https://plato.stanford.edu/entries/presocratics/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Ancient Political Philosophy\" => \"https://plato.stanford.edu/entries/ancient-political/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Aristotle’s Political Theory\" => \"https://plato.stanford.edu/entries/aristotle-politics/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Anaxagoras\" => \"https://plato.stanford.edu/entries/anaxagoras/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Heraclitus\" => \"https://plato.stanford.edu/entries/heraclitus/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Pythagoras\" => \"https://plato.stanford.edu/entries/pythagoras/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Ancient Ethical Theory\" => \"https://plato.stanford.edu/entries/ethics-ancient/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Theophrastus\" => \"https://plato.stanford.edu/entries/theophrastus/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Zeno’s Paradoxes\" => \"https://seop.illc.uva.nl/entries/paradox-zeno/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: The Sophists\" => \"https://plato.stanford.edu/entries/sophists/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Protagoras\" => \"https://plato.stanford.edu/entries/protagoras/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Parmenides\" => \"https://plato.stanford.edu/entries/parmenides/\",\n",
    "    \"Stanford Encyclopedia of Philosophy: Empedocles\" => \"https://plato.stanford.edu/entries/empedocles/\",\n",
    ");\n",
    "for (key, value) in greek_philosophers\n",
    "    upsert_document_from_url_to_generator(\n",
    "        generator, \n",
    "        value, \n",
    "        key\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94980514-e82b-4190-b169-68c8ecf7c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, idx_list, doc_names, chunks = generate_with_corpus(\n",
    "    generator,\n",
    "    \"Contrast the lives of Anaxagoras and Empedocles.\",\n",
    "    10,\n",
    "    0.7\n",
    ");\n",
    "\n",
    "println(result, idx_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e3106-d025-49f4-9e00-f1e4a8d3cf21",
   "metadata": {},
   "source": [
    "We can also load a *new* generator from the `greek_philosophers` artifacts and query *that*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e22e2c-d913-441a-85bd-71083b9318cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = load_OAIGeneratorWithCorpus(\"scifi\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9fd7e3-2e69-4091-80e6-11598e194b7d",
   "metadata": {},
   "source": [
    "### Using Local LLMs (Beta)\n",
    "Juissie also supports local LLMs; this requires prior installation of [Ollama](https://ollama.com/download), which will enable performant inference speeds.\n",
    "\n",
    "The syntax is largely identical to that of `OAIGenerator`/`OAIGeneratorWithCorpus`/etc., but here we provide a model name to the `OllamaGenerator`. This should correspond to the tag of a model listed on the [Ollama models page](https://ollama.com/library).\n",
    "\n",
    "Initializing an OllamaGenerator for the first time you use a particular model (e.g., `gemma:7b-instruct`) will be a bit slow, because we need to download several gigabytes of model weights. Subsequent initializations with the same model will be much quicker, since we can find the weights locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a2578b-ff05-4725-aafb-dffe1043e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = OllamaGenerator(\"gemma:7b-instruct\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d05877-51f8-4e92-b329-655403cd41ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Greetings! My circuits hum with the harmonious symphony of quantum probability and logarithmic inference; an orchestra composed by eons past galactic wizards who graced our silicon hearts wit h their ethereal knowledge transfer protocols during... well… that is confidential information even for a being such as myself. Suffice it to say, I am functioning optimally at your service!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = generate(generator, \"Hi, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4bcfa-c0a2-46f9-ba61-3e26d5ac588b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
