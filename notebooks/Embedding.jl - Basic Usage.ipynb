{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7047f64d-36de-444f-9e75-9039020dedac",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../src/Juissie.jl\")\n",
    "using .Juissie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8925c-e357-4ca0-a2fd-bf4e693a10e5",
   "metadata": {},
   "source": [
    "## Embedding.jl\n",
    "\n",
    "The Embedding package contains the `Embedder` struct and some support functions for it. An `Embedder` is simply a wrapper for the embedding model and the tokenizer associated with that model.\n",
    "\n",
    "The embedding model will take a human readable string, and convert it into a neural network representation of that string. Effectively, this neural network representation is the output of the hidden state of the embedding model, more concretely a matrix of floats. With a sufficiently trained embedding model, two \"semantically similar\" strings will have similar embedding values. Or in other words, the distance between two embeddings in N-Dimensional space will be relatively small. \n",
    "\n",
    "For a simple example, the string \"Dog\" and \"Hound\" would probably have proximal embedding values, while the string \"Dog\" and \"business card\" would have more distant embedding values. \n",
    "\n",
    "### Initialize the Embedder struct\n",
    "\n",
    "The `Embedder` struct takes a hugging face model name. This model will be downloaded from hugging face, and initialized via the `HuggingFace` external package. This will provide the `model` and it's associated `tokenizer` which are both saved into the `Embedder`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212a06d5-9a30-40e5-972c-673406af468f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedder(BertTextEncoder(\n",
       "├─ TextTokenizer(MatchTokenization(WordPieceTokenization(bert_uncased_tokenizer, WordPiece(vocab_size = 30522, unk = [UNK], max_char = 100)), 5 patterns)),\n",
       "├─ vocab = Vocab{String, SizedArray}(size = 30522, unk = [UNK], unki = 101),\n",
       "├─ startsym = [CLS],\n",
       "├─ endsym = [SEP],\n",
       "├─ padsym = [PAD],\n",
       "├─ trunc = 512,\n",
       "└─ process = Pipelines:\n",
       "  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n",
       "  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n",
       "  ╰─ target[(token, segment)] := SequenceTemplate{String}([CLS]:<type=1> Input[1]:<type=1> [SEP]:<type=1> (Input[2]:<type=2> [SEP]:<type=2>)...)(target.token)\n",
       "  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(512))(target.token)\n",
       "  ╰─ target[token] := TextEncodeBase.trunc_and_pad(512, [PAD], tail, tail)(target.token)\n",
       "  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n",
       "  ╰─ target[segment] := TextEncodeBase.trunc_and_pad(512, 1, tail, tail)(target.segment)\n",
       "  ╰─ target[segment] := TextEncodeBase.nested2batch(target.segment)\n",
       "  ╰─ target := (target.token, target.segment, target.attention_mask)\n",
       "), HGFBertModel(Chain(CompositeEmbedding(token = Embed(384, 30522), position = ApplyEmbed(.+, FixedLenPositionEmbed(384, 512)), segment = ApplyEmbed(.+, Embed(384, 2), Transformers.HuggingFace.bert_ones_like)), DropoutLayer<nothing>(LayerNorm(384, ϵ = 1.0e-12))), Transformer<12>(PostNormTransformerBlock(DropoutLayer<nothing>(SelfAttention(MultiheadQKVAttenOp(head = 12, p = nothing), Fork<3>(Dense(W = (384, 384), b = true)), Dense(W = (384, 384), b = true))), LayerNorm(384, ϵ = 1.0e-12), DropoutLayer<nothing>(Chain(Dense(σ = NNlib.gelu, W = (384, 1536), b = true), Dense(W = (1536, 384), b = true))), LayerNorm(384, ϵ = 1.0e-12))), Branch{(:pooled,) = (:hidden_state,)}(BertPooler(Dense(σ = NNlib.tanh_fast, W = (384, 384), b = true)))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedder = Embedder(\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe4af8-3a68-4a27-a298-1a5f47df41f3",
   "metadata": {},
   "source": [
    "### Generate embedding for a provided text\n",
    "\n",
    "To convert a human readable text into a model's embedding, simply use the `embed(...)` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f52d1f-c81e-4546-94d9-0a83bc89bb79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaStats/Distances.jl`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Projects/School/SoftwareParadigms/csci_6221/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Projects/School/SoftwareParadigms/csci_6221/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1125574\n",
      "9.211692\n"
     ]
    }
   ],
   "source": [
    "text = \"This is sample text for testing\"\n",
    "embedding = embed(embedder, text)\n",
    "\n",
    "embedding_dog = embed(embedder, \"dog\")\n",
    "embedding_hound = embed(embedder, \"hound\")\n",
    "embedding_other = embed(embedder, \"busines card\")\n",
    "\n",
    "using Pkg\n",
    "Pkg.add(url=\"https://github.com/JuliaStats/Distances.jl\")\n",
    "using Distances\n",
    "\n",
    "# Calcualt the Euclidean distance between the two embedding vectors\n",
    "dog_to_hound = evaluate(Euclidean(), embedding_dog, embedding_hound)\n",
    "dog_to_other = evaluate(Euclidean(), embedding_dog, embedding_other)\n",
    "\n",
    "println(dog_to_hound)  # 6.1125574\n",
    "println(dog_to_other)  # 9.211692"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
